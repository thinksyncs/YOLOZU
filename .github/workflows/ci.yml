name: ci

on:
  push:
  pull_request:

jobs:
  pip_smoke:
    name: pip smoke (python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install (runtime only)
        run: |
          python -m pip install --upgrade pip
          python -m pip install .

      - name: CLI smoke (doctor + demo)
        run: |
          yolozu doctor --output -
          yolozu demo instance-seg --num-images 2 --image-size 48 --max-instances 2 --run-dir "${RUNNER_TEMP}/yolozu_demo"

  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements-dev.txt
          python -m pip install safetensors

      - name: Packaging smoke (pip install + yolozu doctor)
        run: |
          python -m pip install .
          yolozu doctor --output -
          yolozu export --help
          python -c "from yolozu import resources; paths = resources.list_resource_paths(); assert 'schemas/predictions.schema.json' in paths; assert 'protocols/yolo26_eval.json' in paths; print('resources OK:', len(paths))"

      - name: License policy
        run: |
          python tools/check_license_policy.py

      - name: Lint
        run: |
          ruff check .

      - name: Unit tests
        run: |
          python -m unittest discover -s tests -t tests
          python -m unittest discover -s rtdetr_pose/tests -t rtdetr_pose/tests

        - name: Schema compatibility gate
        run: |
          python - <<'PY'
          import json
          import os
          import subprocess
          import sys
          from pathlib import Path

          repo = Path.cwd()
          out = Path("reports") / "ci_schema_gate"
          out.mkdir(parents=True, exist_ok=True)

          pred_v1 = out / "pred_v1.json"
          pred_v1.write_text(
            json.dumps(
              {
                "schema_version": 1,
                "predictions": [
                  {
                    "image": "000001.jpg",
                    "detections": [
                      {
                        "class_id": 0,
                        "score": 0.9,
                        "bbox": {"cx": 0.5, "cy": 0.5, "w": 0.2, "h": 0.2},
                      }
                    ],
                  }
                ],
              }
            ),
            encoding="utf-8",
          )

          pred_v2 = out / "pred_v2.json"
          pred_v2.write_text(
            json.dumps(
              {
                "schema_version": 2,
                "predictions": [{"image": "000001.jpg", "detections": []}],
              }
            ),
            encoding="utf-8",
          )

          seg_v1 = out / "seg_v1.json"
          seg_v1.write_text(json.dumps({"schema_version": 1, "predictions": [{"id": "0001", "mask": "m.png"}]}), encoding="utf-8")

          seg_v2 = out / "seg_v2.json"
          seg_v2.write_text(json.dumps({"schema_version": 2, "predictions": [{"id": "0001", "mask": "m.png"}]}), encoding="utf-8")

          inst_v1 = out / "inst_v1.json"
          inst_v1.write_text(
            json.dumps(
              {
                "schema_version": 1,
                "predictions": [{"image": "0001.jpg", "instances": [{"class_id": 0, "score": 0.9, "mask": "m.png"}]}],
              }
            ),
            encoding="utf-8",
          )

          inst_v2 = out / "inst_v2.json"
          inst_v2.write_text(
            json.dumps(
              {
                "schema_version": 2,
                "predictions": [{"image": "0001.jpg", "instances": [{"class_id": 0, "score": 0.9, "mask": "m.png"}]}],
              }
            ),
            encoding="utf-8",
          )

          def run_ok(cmd):
            proc = subprocess.run(cmd, cwd=repo, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)
            if proc.returncode != 0:
              raise SystemExit(f"expected success: {' '.join(cmd)}\nSTDOUT:\n{proc.stdout}\nSTDERR:\n{proc.stderr}")

          def run_fail(cmd):
            proc = subprocess.run(cmd, cwd=repo, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)
            if proc.returncode == 0:
              raise SystemExit(f"expected failure: {' '.join(cmd)}\nSTDOUT:\n{proc.stdout}\nSTDERR:\n{proc.stderr}")

          run_ok([sys.executable, "-m", "yolozu.cli", "validate", "predictions", str(pred_v1), "--strict"])
          run_ok([sys.executable, "-m", "yolozu.cli", "validate", "seg", str(seg_v1)])
          run_ok([sys.executable, "-m", "yolozu.cli", "validate", "instance-seg", str(inst_v1)])

          run_fail([sys.executable, "-m", "yolozu.cli", "validate", "predictions", str(pred_v2), "--strict"])
          run_fail([sys.executable, "-m", "yolozu.cli", "validate", "seg", str(seg_v2)])
          run_fail([sys.executable, "-m", "yolozu.cli", "validate", "instance-seg", str(inst_v2)])

          print("schema compatibility gate passed")
          PY

      - name: Migration smoke (migrate + validate)
        run: |
          python - <<'PY'
          import os
          import json
          from pathlib import Path
          from PIL import Image

          out_dir = Path("reports") / "ci_migrate"
          out_dir.mkdir(parents=True, exist_ok=True)

          dataset_root = Path(os.environ["RUNNER_TEMP"]) / "ultra_seg_dataset"
          images = dataset_root / "images" / "val"
          labels = dataset_root / "labels" / "val"
          images.mkdir(parents=True, exist_ok=True)
          labels.mkdir(parents=True, exist_ok=True)
          Image.new("RGB", (32, 32), color=(0, 0, 0)).save(images / "000001.jpg")
          (labels / "000001.txt").write_text("0 0.1 0.1 0.9 0.1 0.9 0.9 0.1 0.9\n", encoding="utf-8")

          data_yaml = Path(os.environ["RUNNER_TEMP"]) / "ultra_data.yaml"
          data_yaml.write_text(
              "\n".join(
                  [
                      f"path: {dataset_root}",
                      "train: images/train",
                      "val: images/val",
                      "task: segment",
                      "",
                  ]
              ),
              encoding="utf-8",
          )
          args_yaml = Path(os.environ["RUNNER_TEMP"]) / "ultra_args.yaml"
          args_yaml.write_text(f"data: {data_yaml}\ntask: segment\n", encoding="utf-8")

          voc_root = Path(os.environ["RUNNER_TEMP"]) / "voc_smoke" / "VOC2012"
          (voc_root / "JPEGImages").mkdir(parents=True, exist_ok=True)
          (voc_root / "SegmentationClass").mkdir(parents=True, exist_ok=True)
          (voc_root / "ImageSets" / "Segmentation").mkdir(parents=True, exist_ok=True)
          (voc_root / "JPEGImages" / "0001.jpg").write_bytes(b"")
          (voc_root / "SegmentationClass" / "0001.png").write_bytes(b"")
          (voc_root / "ImageSets" / "Segmentation" / "val.txt").write_text("0001\n", encoding="utf-8")

          coco_root = Path(os.environ["RUNNER_TEMP"]) / "coco_smoke"
          (coco_root / "images" / "val2017").mkdir(parents=True, exist_ok=True)
          (coco_root / "annotations").mkdir(parents=True, exist_ok=True)
          Image.new("RGB", (64, 32), color=(0, 0, 0)).save(coco_root / "images" / "val2017" / "0001.jpg")
          instances = {
              "images": [{"id": 1, "file_name": "0001.jpg", "width": 64, "height": 32}],
              "annotations": [{"id": 1, "image_id": 1, "category_id": 7, "bbox": [0, 0, 10, 20], "iscrowd": 0}],
              "categories": [{"id": 7, "name": "thing"}],
          }
          (coco_root / "annotations" / "instances_val2017.json").write_text(json.dumps(instances), encoding="utf-8")
          results = [{"image_id": 1, "category_id": 7, "bbox": [0, 0, 10, 20], "score": 0.9}]
          (coco_root / "results.json").write_text(json.dumps(results), encoding="utf-8")
          print("prepared:", dataset_root, data_yaml, args_yaml, voc_root)
          PY

          yolozu migrate dataset \
            --from ultralytics \
            --data "${RUNNER_TEMP}/ultra_data.yaml" \
            --args "${RUNNER_TEMP}/ultra_args.yaml" \
            --output reports/ci_migrate/ultra_wrapper \
            --force

          yolozu validate dataset reports/ci_migrate/ultra_wrapper --split val
          yolozu validate dataset "${RUNNER_TEMP}/ultra_seg_dataset" --split val --label-format segment

          yolozu migrate seg-dataset \
            --from voc \
            --root "${RUNNER_TEMP}/voc_smoke/VOC2012" \
            --split val \
            --output reports/ci_migrate/voc_seg_dataset.json \
            --force

          python - <<'PY'
          import json
          from pathlib import Path
          from yolozu.segmentation_dataset import load_seg_dataset_descriptor

          d = load_seg_dataset_descriptor("reports/ci_migrate/voc_seg_dataset.json")
          assert d.task == "semantic_segmentation"
          assert len(d.samples) == 1

          preds = Path("reports/ci_migrate/seg_preds.json")
          preds.write_text(json.dumps([{"id": "0001", "mask": "pred.png"}]), encoding="utf-8")
          print("ok", preds)
          PY

          yolozu validate seg reports/ci_migrate/seg_preds.json

          yolozu migrate dataset \
            --from coco \
            --coco-root "${RUNNER_TEMP}/coco_smoke" \
            --split val2017 \
            --output reports/ci_migrate/coco_wrapper \
            --force

          yolozu validate dataset reports/ci_migrate/coco_wrapper --split val2017

          yolozu migrate predictions \
            --from coco-results \
            --results "${RUNNER_TEMP}/coco_smoke/results.json" \
            --instances "${RUNNER_TEMP}/coco_smoke/annotations/instances_val2017.json" \
            --output reports/ci_migrate/coco_predictions.json \
            --force

          yolozu validate predictions reports/ci_migrate/coco_predictions.json --strict

          yolozu import dataset \
            --from coco-instances \
            --instances "${RUNNER_TEMP}/coco_smoke/annotations/instances_val2017.json" \
            --images-dir "${RUNNER_TEMP}/coco_smoke/images/val2017" \
            --split val2017 \
            --output reports/ci_migrate/coco_import_wrapper \
            --force

          yolozu validate dataset reports/ci_migrate/coco_import_wrapper --split val2017

      - name: Upload migration artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ci_migrate_artifacts
          path: reports/ci_migrate

      - name: Scenario smoke run
        run: |
          if bash tools/fetch_coco128.sh; then
            python tools/run_scenarios.py --adapter dummy --max-images 10
          else
            echo "warning: coco128 fetch failed; running dataset-free scenario suite smoke"
            python tools/run_scenario_suite.py --output reports/scenario_suite_ci.json
          fi

      - name: Benchmark publication smoke
        run: |
          python - <<'PY'
          import json
          from pathlib import Path

          reports = Path("reports")
          reports.mkdir(parents=True, exist_ok=True)

          a = {
              "schema_version": 1,
              "timestamp": "2026-01-01T00:00:00Z",
              "metrics": {
                  "buckets": [
                      {"name": "yolo26n", "metrics": {"fps": 40.0, "latency_ms": {"mean": 25.0}}},
                      {"name": "yolo26s", "metrics": {"fps": 30.0, "latency_ms": {"mean": 33.0}}},
                  ]
              },
              "meta": {"run_id": "ci_bench_a", "git_head": "deadbeef"},
          }
          b = {
              "schema_version": 1,
              "timestamp": "2026-01-01T00:01:00Z",
              "metrics": {"summary": {"fps_mean": 20.0, "latency_ms_mean": 50.0}},
              "meta": {"run_id": "ci_bench_b", "git_head": "deadbeef"},
          }
          (reports / "benchmark_latency_ci_a.json").write_text(json.dumps(a), encoding="utf-8")
          (reports / "benchmark_latency_ci_b.json").write_text(json.dumps(b), encoding="utf-8")
          PY

          python tools/publish_benchmark_table.py \
            --report reports/benchmark_latency_ci_a.json \
            --report reports/benchmark_latency_ci_b.json \
            --output-json reports/benchmark_table_ci.json \
            --output-md reports/benchmark_table_ci.md \
            --source-command "python3 tools/benchmark_latency.py --config configs/benchmark_latency_example.json"

          test -f reports/benchmark_table_ci.json
          test -f reports/benchmark_table_ci.md
          python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("reports/benchmark_table_ci.json").read_text(encoding="utf-8"))
          rows = payload.get("rows") or []
          run_ids = {str(r.get("run_id")) for r in rows}
          assert "ci_bench_a" in run_ids and "ci_bench_b" in run_ids
          assert payload.get("protocol", {}).get("id") == "yolo26"
          print("benchmark publication smoke passed")
          PY

      - name: Training contract smoke (dry-run + ONNX export/parity)
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          from PIL import Image
          root = Path(os.environ["RUNNER_TEMP"]) / "yolozu_tiny_dataset"
          for split in ("train2017", "val2017"):
              images = root / "images" / split
              labels = root / "labels" / split
              images.mkdir(parents=True, exist_ok=True)
              labels.mkdir(parents=True, exist_ok=True)
              for idx in range(2):
                  name = f"{idx:06d}"
                  img_path = images / f"{name}.jpg"
                  Image.new("RGB", (32, 32), color=(0, 0, 0)).save(img_path)
                  (labels / f"{name}.txt").write_text("0 0.5 0.5 0.2 0.2\n", encoding="utf-8")
          print(root)
          PY

          yolozu train configs/examples/train_contract.yaml \
            --run-id ci_smoke \
            --dry-run \
            --dataset-root "${RUNNER_TEMP}/yolozu_tiny_dataset" \
            --split train2017 \
            --val-split val2017 \
            --device cpu \
            --amp none \
            --ddp-backend gloo \
            --image-size 32 \
            --batch-size 1

          test -f runs/ci_smoke/reports/run_meta.json
          test -f runs/ci_smoke/reports/config_resolved.yaml
          test -f runs/ci_smoke/checkpoints/last.pt
          test -f runs/ci_smoke/checkpoints/best.pt
          test -f runs/ci_smoke/exports/model.onnx
          test -f runs/ci_smoke/reports/onnx_parity.json

          python - <<'PY'
          import json
          from pathlib import Path
          from yolozu.run_record import validate_run_record_contract

          meta = json.loads(Path("runs/ci_smoke/reports/run_meta.json").read_text(encoding="utf-8"))
          validate_run_record_contract(meta, require_git_sha=True)
          assert "dependency_lock" in meta and isinstance(meta["dependency_lock"], dict)
          assert "preprocess" in meta and isinstance(meta["preprocess"], dict)
          assert "command" in meta and isinstance(meta["command"], dict)
          print("run_meta keys OK")
          PY

          - name: Backend parity matrix smoke (JSON + HTML)
          run: |
            python - <<'PY'
            import json
            import os
            import subprocess
            import sys
            from pathlib import Path
            from PIL import Image

            root = Path(os.environ["RUNNER_TEMP"]) / "backend_parity_matrix_smoke"
            root.mkdir(parents=True, exist_ok=True)
            image = root / "000001.jpg"
            Image.new("RGB", (8, 8), color=(0, 0, 0)).save(image)

            payload = [
              {
                "image": str(image.resolve()),
                "detections": [
                  {
                    "class_id": 0,
                    "score": 0.9,
                    "bbox": {"cx": 0.5, "cy": 0.5, "w": 0.2, "h": 0.2},
                  }
                ],
              }
            ]

            args = []
            for backend in ("torch", "onnxrt", "trt", "opencv_dnn", "custom_cpp"):
              path = root / f"pred_{backend}.json"
              path.write_text(json.dumps(payload), encoding="utf-8")
              args.extend(["--backend-predictions", f"{backend}={path}"])

            run_dir = Path("runs") / "backend_parity_matrix" / "ci_smoke"
            proc = subprocess.run(
              [
                sys.executable,
                "tools/backend_parity_matrix.py",
                *args,
                "--reference-backend",
                "torch",
                "--image-size",
                "8,8",
                "--run-dir",
                str(run_dir),
              ],
              stdout=subprocess.PIPE,
              stderr=subprocess.PIPE,
              text=True,
              check=False,
            )
            if proc.returncode != 0:
              raise SystemExit(f"backend parity matrix smoke failed\nSTDOUT:\n{proc.stdout}\nSTDERR:\n{proc.stderr}")

            report_json = run_dir / "reports" / "backend_parity_matrix.json"
            report_html = run_dir / "reports" / "backend_parity_matrix.html"
            if not report_json.is_file() or not report_html.is_file():
              raise SystemExit(f"missing outputs: {report_json} {report_html}")

            report = json.loads(report_json.read_text(encoding="utf-8"))
            if not report.get("ok"):
              raise SystemExit("backend parity matrix smoke produced non-ok report")
            if len(report.get("matrix") or []) != 4:
              raise SystemExit("backend parity matrix smoke report has unexpected matrix size")
            print("backend parity matrix smoke passed")
            PY

      - name: PyInstaller smoke (build + resources list)
        run: |
          python -m pip install pyinstaller
          pyinstaller -y -n yolozu_bin \
            deploy/pyinstaller/yolozu_entrypoint.py \
            --collect-data yolozu.data \
            --collect-data rtdetr_pose
          ./dist/yolozu_bin/yolozu_bin --help
          ./dist/yolozu_bin/yolozu_bin resources list | head -n 5

      - name: PyArmor + PyInstaller smoke (optional)
        if: ${{ vars.PYARMOR_SMOKE == '1' }}
        run: |
          python -m pip install pyarmor
          rm -rf build/pyarmor_obf dist/yolozu_pyarmor yolozu_pyarmor.spec
          pyarmor gen -O build/pyarmor_obf deploy/pyinstaller/yolozu_entrypoint.py
          pyinstaller -y -n yolozu_pyarmor \
            build/pyarmor_obf/yolozu_entrypoint.py \
            --collect-data yolozu.data \
            --collect-data rtdetr_pose
          ./dist/yolozu_pyarmor/yolozu_pyarmor --help
