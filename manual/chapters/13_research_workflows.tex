\chapter{Research Workflows (End-to-end Loops)}

\section{The research loop in \yolozu{}}
A typical loop is:
\begin{enumerate}
  \item Build or obtain a model (train or external).
  \item Export predictions into the stable JSON contract.
  \item Validate contracts (fail fast).
  \item Evaluate under a pinned protocol (suite reports).
  \item Compare backends (parity) and compare runs (history JSONL/CSV/MD).
  \item (Optional) post-hoc calibration, re-evaluate.
  \item (Optional) sweep parameters and benchmark latency.
\end{enumerate}

The key discipline is: \textbf{make artifacts first-class}. A run should produce JSON files that
let you reproduce and compare.

\section{Freeze a subset dataset for ablations}
For fast ablations you often want a deterministic subset of a dataset.
\yolozu{} provides a helper:
\begin{lstlisting}[language=bash]
python3 tools/make_subset_dataset.py \
  --dataset data/coco128 \
  --split train2017 \
  --n 50 \
  --seed 0 \
  --out reports/coco128_50
\end{lstlisting}

This produces:
\begin{itemize}
  \item a subset dataset directory,
  \item a frozen image list (for reproducible sampling),
  \item a subset metadata JSON (for provenance).
\end{itemize}

\section{Protocol-pinned evaluation (YOLO26 example)}
Protocols pin evaluation settings so numbers are comparable over time.
For YOLO26, see \path{docs/yolo26_eval_protocol.md} and \path{protocols/yolo26_eval.json}.

Run a suite over many prediction artifacts:
\begin{lstlisting}[language=bash]
python3 tools/eval_suite.py \
  --protocol yolo26 \
  --dataset /path/to/yolo-dataset \
  --predictions-glob '/path/to/pred_*.json' \
  --output reports/eval_suite.json
\end{lstlisting}

The report includes protocol metadata (split, bbox format, etc.), which is essential for
research comparisons.

\section{Validate contracts as a research gate}
Before spending time interpreting metrics, validate artifacts:
\begin{lstlisting}[language=bash]
python3 tools/validate_dataset.py --dataset /path/to/yolo --strict
python3 tools/validate_predictions.py /path/to/predictions.json --strict
\end{lstlisting}

This is especially important when you export predictions from external pipelines.

\section{Backend parity (torch vs onnxrt vs trt)}
When comparing inference backends, first ensure the outputs are aligned.
Use parity tooling (see \path{README.md} and \path{docs/onnx_export_parity.md}).

Conceptual usage:
\begin{lstlisting}[language=bash]
yolozu parity \
  --reference reports/pred_torch.json \
  --candidate reports/pred_onnxrt.json
\end{lstlisting}

\section{Sweeps (hyperparameter search harness)}
Research frequently needs parameter sweeps even outside the main training loop.
The sweep harness executes external commands, collects metrics, and writes tables.

Quick start:
\begin{lstlisting}[language=bash]
python3 tools/hpo_sweep.py --config docs/hpo_sweep_example.json --resume
\end{lstlisting}

Outputs (by default):
\begin{itemize}
  \item \path{reports/hpo_sweep.jsonl}
  \item \path{reports/hpo_sweep.csv}
  \item \path{reports/hpo_sweep.md}
\end{itemize}

Use \cmd{--resume} to make sweeps incremental and robust to interruptions.

\section{Latency/FPS benchmarks with history (JSONL)}
Benchmark reports are also artifacts.
The latency harness emits a stable JSON report and can append to a JSONL history:
\begin{lstlisting}[language=bash]
python3 tools/benchmark_latency.py \
  --iterations 200 \
  --warmup 20 \
  --output reports/benchmark_latency.json \
  --history reports/benchmark_latency.jsonl \
  --notes 'baseline on target HW'
\end{lstlisting}

This pattern (single JSON + JSONL history) is useful for tracking regressions across commits and
hardware.

\section{Run contract artifacts (training research)}
If you use the in-repo trainer, prefer the contracted run mode for consistent artifacts.
See \path{docs/run_contract.md}.

Minimal example:
\begin{lstlisting}[language=bash]
yolozu train configs/examples/train_contract.yaml --run-id exp01
\end{lstlisting}

Contracted outputs live under \path{runs/exp01/} and include checkpoints, metrics JSONL, ONNX
exports, and parity reports.

\section{Notes on long-tail calibration experiments}
Post-hoc calibration is often a research axis. Use \cmd{yolozu calibrate} to generate calibrated
prediction artifacts and compare methods side-by-side.
See \path{docs/score_calibration.md}.
