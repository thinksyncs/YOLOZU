\chapter{Research Workflows (End-to-end Loops)}

\section{The research loop in \yolozu{}}
A typical loop is:
\begin{enumerate}
  \item Build or obtain a model (train or external).
  \item Export predictions into the stable JSON contract.
  \item Validate contracts (fail fast).
  \item Evaluate under a pinned protocol (suite reports).
  \item Compare backends (parity) and compare runs (history JSONL/CSV/MD).
  \item (Optional) post-hoc calibration, re-evaluate.
  \item (Optional) sweep parameters and benchmark latency.
\end{enumerate}

The key discipline is: \textbf{make artifacts first-class}. A run should produce JSON files that
let you reproduce and compare.

\section{Freeze a subset dataset for ablations}
For fast ablations you often want a deterministic subset of a dataset.
\yolozu{} provides a helper:
\begin{lstlisting}[language=bash]
python3 tools/make_subset_dataset.py \
  --dataset data/coco128 \
  --split train2017 \
  --n 50 \
  --seed 0 \
  --out reports/coco128_50
\end{lstlisting}

This produces:
\begin{itemize}
  \item a subset dataset directory,
  \item a frozen image list (for reproducible sampling),
  \item a subset metadata JSON (for provenance).
\end{itemize}

\section{Protocol-pinned evaluation (YOLO26 example)}
Protocols pin evaluation settings so numbers are comparable over time.
For YOLO26, the protocol is defined by \path{protocols/yolo26_eval.json} (the tool reads it and records it into suite reports).

Run a suite over many prediction artifacts:
\begin{lstlisting}[language=bash]
python3 tools/eval_suite.py \
  --protocol yolo26 \
  --dataset /path/to/yolo-dataset \
  --predictions-glob '/path/to/pred_*.json' \
  --output reports/eval_suite.json
\end{lstlisting}

The report includes protocol metadata (split, bbox format, etc.), which is essential for
research comparisons.

\section{Validate contracts as a research gate}
Before spending time interpreting metrics, validate artifacts:
\begin{lstlisting}[language=bash]
python3 tools/validate_dataset.py --dataset /path/to/yolo --strict
python3 tools/validate_predictions.py /path/to/predictions.json --strict
\end{lstlisting}

This is especially important when you export predictions from external pipelines.

\section{Backend parity (torch vs onnxrt vs trt)}
When comparing inference backends, first ensure the outputs are aligned.
Use parity tooling to compare two \path{predictions.json} artifacts and flag systematic mismatches (preprocess, coordinate formats, numerics).

Conceptual usage:
\begin{lstlisting}[language=bash]
yolozu parity \
  --reference reports/pred_torch.json \
  --candidate reports/pred_onnxrt.json
\end{lstlisting}

\section{Sweeps (hyperparameter search harness)}
Research frequently needs parameter sweeps even outside the main training loop.
The sweep harness executes external commands, collects metrics, and writes tables.

Quick start:
\begin{lstlisting}[language=bash]
python3 tools/hpo_sweep.py --config docs/hpo_sweep_example.json --resume
\end{lstlisting}

Outputs (by default):
\begin{itemize}
  \item \path{reports/hpo_sweep.jsonl}
  \item \path{reports/hpo_sweep.csv}
  \item \path{reports/hpo_sweep.md}
\end{itemize}

Use \cmd{--resume} to make sweeps incremental and robust to interruptions.

Important config fields (from \path{docs/hpo_sweep.md}):
\begin{itemize}
  \item required: \cmd{base\_cmd}, and either \cmd{param\_grid} or \cmd{param\_list}
  \item optional: \cmd{run\_dir}, \cmd{metrics.path}, \cmd{metrics.keys}
  \item optional outputs: \cmd{result\_jsonl}, \cmd{result\_csv}, \cmd{result\_md}
  \item optional runtime controls: \cmd{env}, \cmd{shell}
\end{itemize}

The harness is framework-agnostic: it does not assume a specific training stack.

\section{Prediction distillation workflow}
\yolozu{} includes a lightweight distillation helper at
\path{tools/distill_predictions.py}.

Canonical usage:
\begin{lstlisting}[language=bash]
python3 tools/distill_predictions.py \
  --student reports/predictions_student.json \
  --teacher reports/predictions_teacher.json \
  --dataset data/coco128 \
  --output reports/predictions_distilled.json \
  --output-report reports/distill_report.json \
  --add-missing
\end{lstlisting}

Distillation config keys include:
\begin{itemize}
  \item \cmd{enabled}
  \item \cmd{iou\_threshold}
  \item \cmd{alpha}
  \item \cmd{add\_missing}
  \item \cmd{add\_score\_scale}
\end{itemize}

Notes:
\begin{itemize}
  \item The helper blends teacher predictions into student predictions and emits a small distillation report JSON.
  \item Some quick comparisons use a lightweight \cmd{yolozu.simple_map} proxy on a fixed subset (fast CPU feedback), and you should follow up with the full evaluator when reporting results.
\end{itemize}

In research loops, this is useful for quick teacher-student ablations using fixed subsets and
the same downstream evaluation protocol.

\section{Latency/FPS benchmarks with history (JSONL)}
Benchmark reports are also artifacts.
The latency harness emits a stable JSON report and can append to a JSONL history:
\begin{lstlisting}[language=bash]
python3 tools/benchmark_latency.py \
  --iterations 200 \
  --warmup 20 \
  --output reports/benchmark_latency.json \
  --history reports/benchmark_latency.jsonl \
  --notes 'baseline on target HW'
\end{lstlisting}

This pattern (single JSON + JSONL history) is useful for tracking regressions across commits and
hardware.

\section{Run contract artifacts (training research)}
If you use the in-repo trainer, prefer the contracted run mode for consistent artifacts.
This is summarized in Chapter~7 (Training run contract).

Minimal example:
\begin{lstlisting}[language=bash]
yolozu train configs/examples/train_contract.yaml --run-id exp01
\end{lstlisting}

Contracted outputs live under \path{runs/exp01/} and include checkpoints, metrics JSONL, ONNX
exports, and parity reports.

\section{Notes on long-tail calibration experiments}
Post-hoc calibration is often a research axis. Use \cmd{yolozu calibrate} to generate calibrated
prediction artifacts and compare methods side-by-side.
This is summarized in Chapter~6 (Score calibration and long-tail adjustments).
