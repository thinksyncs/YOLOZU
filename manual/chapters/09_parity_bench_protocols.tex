\chapter{Parity, Benchmarks, and Protocols}

\ChapterMeta{
This chapter details parity checking methodologies, protocol-pinned evaluation configurations, and the utilization of benchmark and sweep harnesses.
}{
These tools ensure that comparisons remain stable and actionable by proactively detecting silent numerical drift and systematically tracking performance and metric trends over time.
}{
Execution requires a minimum of two comparable prediction artifacts for parity analysis; protocol-driven runs necessitate pinned settings files alongside consistent preprocessing and output formats.
}

\section{Backend parity}
When exporting predictions across disparate backends (e.g., PyTorch, ONNX Runtime, TensorRT), it is critical to detect any silent numerical drift. Parity tooling systematically compares two prediction artifacts and generates a comprehensive report of any discrepancies.

\textbf{Conceptual (not copy-paste as-is):} parity command shape.
For a runnable shortest command, use Chapter~5 Workflow A with real artifact paths.
\begin{lstlisting}[language=bash]
yolozu parity   --reference reports/pred_torch.json   --candidate reports/pred_onnxrt.json
\end{lstlisting}

\section{Protocols (pinning evaluation settings)}
Protocols establish immutable evaluation configurations (such as dataset splits, image dimensions, and specific metric keys). This standardization is paramount for ensuring stable, longitudinal comparisons.

For YOLO26 comparisons, the pinned protocol is defined as follows:
\begin{itemize}
  \item Protocol file: \path{protocols/yolo26_eval.json}
  \item Task: \texttt{detect}, Dataset: \texttt{COCO}, Split: \texttt{val2017}
  \item Image size: \texttt{640} (enforcement is the responsibility of the export pipeline)
  \item Bounding box format: \texttt{cxcywh\_norm}
  \item Primary score key: \texttt{map50\_95}
\end{itemize}

A critical semantic rule derived from the protocol documentation:
\begin{itemize}
  \item "End-to-end (e2e) mAP" dictates that the evaluator scores detections exactly as provided; it explicitly does \textit{not} apply Non-Maximum Suppression (NMS).
\end{itemize}

\section{Benchmarks}
Latency and FPS benchmarks constitute a vital component of the regression testing surface.

The benchmark harness, located at \path{tools/benchmark_latency.py}, generates:
\begin{itemize}
  \item A stable JSON report (detailing a single run).
  \item A JSONL history file (facilitating append-only trend tracking).
\end{itemize}

This harness supports both synthetic timing evaluations and configuration-driven, per-bucket executions. In the latter mode, each bucket can ingest externally measured metrics via JSON (for instance, data generated by TensorRT latency measurement scripts).

Typically tracked metrics include FPS and latency quantiles (mean, p50, p90, p95, and p99).

\section{Sweeps}
A sweep harness is indispensable for systematically comparing numerous runs or configurations.

The sweep runner, \path{tools/hpo_sweep.py}, is entirely framework-agnostic and operates by executing templated commands. The core configuration model comprises:
\begin{itemize}
  \item \texttt{base\_cmd}: The command template containing \texttt{\{param\}} placeholders.
  \item \texttt{param\_grid} or \texttt{param\_list}: The defined hyperparameter search space.
  \item Optional keys for metrics extraction and designated output destinations.
\end{itemize}

The default artifacts are designed for effortless diffing and publication:
\path{reports/hpo_sweep.jsonl}, \path{reports/hpo_sweep.csv}, and \path{reports/hpo_sweep.md}.
Utilize the \cmd{--resume} flag to intelligently bypass previously completed parameter combinations.
