\chapter{Parity, Benchmarks, and Protocols}

\ChapterMeta{
This chapter covers parity checking, protocol-pinned evaluation settings, and benchmark and sweep harnesses.
}{
It makes comparisons stable and actionable by detecting silent numeric drift and tracking performance and metrics trends over time.
}{
It requires at least two comparable prediction artifacts for parity; protocol runs require pinned settings files and consistent preprocessing and output formats.
}

\section{Backend parity}
When exporting predictions from different backends (torch vs onnxrt vs trt), you want to detect
silent drift.
Parity tooling compares two predictions artifacts and reports diffs.

Conceptual usage:
\begin{lstlisting}[language=bash]
yolozu parity \
  --reference reports/pred_torch.json \
  --candidate reports/pred_onnxrt.json
\end{lstlisting}

\section{Protocols (pin evaluation settings)}
Protocols define fixed evaluation settings (split, image size, metric keys, etc.).
This enables stable comparisons over time.

For YOLO26 comparison, the pinned protocol is:
\begin{itemize}
  \item protocol file: \path{protocols/yolo26_eval.json}
  \item task: detect, dataset: COCO, split: \texttt{val2017}
  \item image size: \texttt{640} (export side responsibility)
  \item bbox format: \texttt{cxcywh\_norm}
  \item score key: \texttt{map50\_95}
\end{itemize}

Important semantic rule from the protocol docs:
\begin{itemize}
  \item e2e mAP means evaluator scores detections as provided; it does not apply NMS.
\end{itemize}

\section{Benchmarks}
Latency/FPS benchmarks are part of the regression surface.

The benchmark harness \path{tools/benchmark_latency.py} writes:
\begin{itemize}
  \item stable JSON report (single run),
  \item JSONL history (append-only trend tracking).
\end{itemize}

It supports both synthetic timing and config-driven per-bucket runs, where each bucket can load
external measured metrics JSON (for example from TensorRT latency measurement scripts).

Typical tracked fields include FPS and latency quantiles (mean/p50/p90/p95/p99).

\section{Sweeps}
A sweep harness helps compare many runs/configs consistently.

The sweep runner \path{tools/hpo_sweep.py} is framework-agnostic and executes templated commands.
Core config model:
\begin{itemize}
  \item \texttt{base\_cmd}: command template with \texttt{\{param\}} placeholders,
  \item \texttt{param\_grid} or \texttt{param\_list}: search space,
  \item optional metrics extraction keys and output destinations.
\end{itemize}

Default artifacts are easy to diff and publish:
\path{reports/hpo_sweep.jsonl}, \path{reports/hpo_sweep.csv}, \path{reports/hpo_sweep.md}.
Use \cmd{--resume} to skip already completed parameter combinations.
