\chapter{Tasks: Detection, Segmentation, Keypoints}

\ChapterMeta{
This chapter defines the supported task families and their expected prediction shapes.
}{
It prevents schema mismatches and helps ensure the right evaluator is used for each task, improving metric correctness and debuggability.
}{
It assumes task-specific fields are present (for example PNG masks for instance segmentation, and keypoint arrays for pose); validate artifacts early with the provided tools.
}

\section{Bounding-box detection}
BBox detection is the baseline task: per-image detections with class IDs, boxes, and scores.
Evaluation is typically COCO-style mAP or protocol-pinned metrics.

\section{Instance segmentation (PNG masks)}
\yolozu{} evaluates instance segmentation using \textbf{per-instance binary PNG masks}.
This avoids complex RLE/polygon tooling.

Minimal predictions shape:
\begin{lstlisting}[language=json]
[
  {
    "image": "000001.png",
    "instances": [
      {"class_id": 0, "score": 0.9, "mask": "masks/000001_inst0.png"}
    ]
  }
]
\end{lstlisting}

Validate:
\begin{lstlisting}[language=bash]
python3 tools/validate_instance_segmentation_predictions.py \
  reports/instance_seg_predictions.json
\end{lstlisting}

Evaluate with overlays/HTML:
\begin{lstlisting}[language=bash]
python3 tools/eval_instance_segmentation.py \
  --dataset /path/to/dataset \
  --split val \
  --predictions /path/to/predictions.json \
  --pred-root /path/to/pred-root \
  --classes /path/to/classes.txt \
  --html reports/instance_seg_eval.html \
  --overlays-dir reports/instance_seg_overlays \
  --max-overlays 10
\end{lstlisting}

\section{Semantic segmentation}
Semantic segmentation utilities include dataset preparation helpers and mIoU evaluation.
Semantic segmentation evaluation reports mIoU (and per-class IoU); instance segmentation evaluation reports COCO-style mask mAP.
Use \path{tools/eval_segmentation.py} for semantic seg and \path{tools/eval_instance_segmentation.py} for instance seg.

\section{Keypoints / pose}
\yolozu{} supports YOLO pose-style keypoints in labels and predictions, plus evaluation utilities.
Depending on your workflow, you may use PCK evaluation and/or COCO OKS mAP.

Dataset preparation is available via one command:
\begin{lstlisting}[language=bash]
python3 tools/yolozu.py prepare-keypoints-dataset \
  --source /path/to/export \
  --format auto \
  --out /path/to/keypoints_dataset
\end{lstlisting}

Supported direct inputs: \cmd{auto}, \cmd{yolo_pose}, \cmd{coco}, \cmd{cvat_xml}.
For CVAT XML, pass either an XML file or a directory containing \path{annotations.xml};
when image root inference is ambiguous, set \cmd{--cvat-images-dir} explicitly.

Minimal CVAT XML smoke test:
\begin{lstlisting}[language=bash]
python3 -m pytest -q tests/test_prepare_keypoints_dataset_cvat_xml.py
\end{lstlisting}

Keypoint evaluation entry points include scripts under \path{tools/} (for example, keypoints eval
helpers).
See the repository documentation for the authoritative schema and usage.
