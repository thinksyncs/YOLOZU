\chapter{Hessian-based Refinement (Practical)}

\ChapterMeta{
This chapter introduces per-detection numerical refinement utilizing a Hessian-based Gauss--Newton style solver, applied post-inference.
}{
This technique can significantly mitigate errors on challenging samples during offline analysis and controlled studies by applying iterative, localized corrections.
}{
It is optimally suited for offline and batch processing workflows where additional computational overhead is permissible. It requires both predictions and dataset context, and relies on configurable convergence and damping parameters.
}

\section{Scope and relation to Test-Time Training (TTT)}
This chapter is strictly focused on per-detection numerical refinement executed after the primary inference phase. TTT, Tent, and MIM adaptation policies are comprehensively covered in Chapter~15 to prevent redundant guidance.

\section{Mechanics of the Hessian solver}
The Hessian solver refines prediction values on a per-detection basis, employing Gauss--Newton style updates stabilized by Levenberg--Marquardt damping.

The current standard CLI implementation operates as an engine-external post-processing step applied directly to predictions JSON artifacts, with bounding box offset refinement serving as the initial target.

At a high level, each iteration computes the residuals and the Jacobian matrix, solves a damped normal equation, updates the parameters, and terminates once predefined convergence criteria are satisfied.

\section{Optimal use cases}
Recommended applications include:
\begin{itemize}
  \item Offline validation and analysis scenarios where Ground Truth (GT) supervision or strict geometric constraints are accessible.
  \item Targeted error reduction specifically focused on highly difficult or anomalous samples.
  \item Post-inference refinement studies conducted subsequent to global calibration efforts.
\end{itemize}

It is strongly advised to avoid this technique by default in strict, real-time production environments due to the inherent latency overhead it introduces.

\section{CLI quickstart}
To enable refinement (opt-in behavior):
\begin{lstlisting}[language=bash]
python3 tools/refine_predictions_hessian.py \
  --predictions reports/predictions.json \
  --dataset data/smoke \
  --output reports/predictions_refined.json \
  --enable \
  --refine-offsets \
  --wrap
\end{lstlisting}

To explicitly disable refinement (pass-through behavior):
\begin{lstlisting}[language=bash]
python3 tools/refine_predictions_hessian.py \
  --predictions reports/predictions.json \
  --output reports/predictions_refined.json \
  --disable \
  --wrap
\end{lstlisting}

To enable refinement with customized solver parameters:
\begin{lstlisting}[language=bash]
python3 tools/refine_predictions_hessian.py \
  --predictions reports/predictions.json \
  --dataset data/smoke \
  --output reports/predictions_refined.json \
  --enable \
  --refine-offsets \
  --steps 10 \
  --damping 1e-2
\end{lstlisting}

\subsection{Workflow (misuse-prevention view)}
\begin{enumerate}
  \item Input predictions: \path{reports/predictions.json}
  \item Gate selection: \cmd{--disable} $\rightarrow$ pass-through, \cmd{--enable} $\rightarrow$ iterative refine
  \item Output predictions: \path{reports/predictions_refined.json}
  \item Validate output contract: \cmd{yolozu validate predictions reports/predictions_refined.json --strict}
\end{enumerate}

\section{Core configuration parameters}
Commonly tuned parameters (which correspond to the documentation in \path{docs/hessian_solver.md}):
\begin{itemize}
  \item \textbf{Gate/Control}: \cmd{--enable}/\cmd{--disable}, \cmd{steps}, \cmd{damping}, \cmd{fd_eps}
  \item \textbf{Target toggles}: \cmd{refine_offsets} (prioritizing offsets-first rollout)
  \item \textbf{Residual weights}: \cmd{w_depth}, \cmd{w_mask}, \cmd{w_reg}
  \item \textbf{Configuration file}: \cmd{--config <yaml|json>} (Note: CLI arguments will override config file values)
\end{itemize}

\section{Integration within the broader workflow}
A highly practical operational sequence is:
\begin{enumerate}
  \item Train the core model.
  \item Optionally execute dataset-wide calibration (addressing global scale and intrinsics).
  \item Apply Hessian refinement exclusively in scenarios where per-detection correction is demonstrably required.
\end{enumerate}

This chapter highlights the most frequently utilized parameters; for an exhaustive detailing of the parameter surface, please consult the tool's \cmd{--help} output, which is rigorously maintained in sync with the codebase.


\noindent\textbf{TensorRT Note:} TensorRT conversion strictly targets the inference graph execution. Hessian refinement functions as a completely separate, engine-external post-processing operation.
