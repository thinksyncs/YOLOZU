\chapter{Real-time vs Batch Inference (Research and Deployment)}

\ChapterMeta{
This chapter compares batch and real-time operating modes and outlines the canonical TensorRT pipeline with parity and benchmark safeguards.
}{
It guides you to high-throughput deployment paths while keeping artifact contracts and drift detection in place.
}{
Real-time TensorRT paths are typically Linux with an NVIDIA GPU; parity requires consistent preprocessing and output formats, and expensive extras like TTT and refinement should be explicitly budgeted.
}

\section{Two operating modes}
\begin{description}
  \item[Batch/offline] Run over a dataset split, export \path{predictions.json}, then evaluate and plot.
  \item[Real-time/streaming] Optimize end-to-end latency and stability; avoid expensive per-frame extras.
\end{description}

\section{Backend choices}
\begin{itemize}
  \item \textbf{Precomputed predictions}: best for integration; works on macOS; use \path{tools/run_scenarios.py} with \cmd{precomputed} adapter.
  \item \textbf{Torch}: flexible for research (TTT, ablations), but slower and less deployable.
  \item \textbf{ONNX Runtime}: CPU-friendly parity reference; useful in CI.
  \item \textbf{TensorRT}: preferred for real-time throughput on Linux+NVIDIA.
\end{itemize}

References:
\begin{itemize}
  \item \path{docs/real_model_interface.md}
  \item \path{docs/tensorrt_pipeline.md}
\end{itemize}

\section{TensorRT pipeline (canonical)}
The typical export route is PyTorch $\rightarrow$ ONNX $\rightarrow$ TensorRT (engine build), with parity checks.

Canonical pattern (artifacts-first):
\begin{itemize}
  \item Export ONNX (+ metadata JSON): \cmd{python3 tools/export_trt.py --onnx ... --opset 18 --dynamic-hw ...}
  \item Build engine (+ metadata JSON): \cmd{python3 tools/build_trt_engine.py --onnx ... --engine ... --precision fp16}
  \item Export \path{predictions.json} from TensorRT and run parity vs ONNXRuntime predictions.
\end{itemize}

The main rule for parity is to keep preprocessing and output formats identical (letterbox/RGB, box format, score scaling), and keep NMS disabled in exported graphs when comparing raw outputs.

\section{Stage 7: Performance + deployment safeguards (implemented tooling)}
Stage 7 is about keeping the real-time path fast and predictable.
In this repo, the main safeguards are implemented as \textbf{tooling and contracts} rather than as model-internal graph logic.

\subsection{Keep symmetry/commonsense logic out of TensorRT graphs}
The TensorRT export/build pipeline is designed to focus on the model forward pass.
Symmetry, template scoring, commonsense constraints, and gates are intended to run in \textbf{postprocess} where possible.

In particular:
\begin{itemize}
  \item Commonsense constraints live in \path{yolozu/constraints.py} and are toggled via \path{configs/runtime/constraints.yaml}.
  \item Parity comparisons are performed on raw outputs (no-NMS exports) so numerical mismatches are detectable.
\end{itemize}

\subsection{Benchmark and regression surfaces}
There are two complementary benchmark surfaces:
\begin{itemize}
  \item A backend-agnostic latency/FPS harness: \path{tools/benchmark_latency.py}
  \item A TensorRT engine latency measurement tool (Linux+NVIDIA + TRT python): \path{tools/measure_trt_latency.py}
\end{itemize}

The harness supports optional targets and can fail the run if targets are not met:
\begin{lstlisting}[language=bash]
python tools/benchmark_latency.py --config configs/benchmark_latency_example.json \
  --target-fps 30
\end{lstlisting}

For a full end-to-end TensorRT run on a Linux+NVIDIA box, use:\\
\path{tools/run_trt_pipeline.py} (export/predictions/parity/eval/latency/benchmark; supports \cmd{--dry-run} for CI-style artifact checks).

\subsection{Gate checks against a baseline}
The repo includes a lightweight gate-check script that compares a current scenario suite report with a saved baseline:
\begin{itemize}
  \item \path{tools/check_gates.py} (writes \path{reports/gate_check.json})
  \item baseline source: \path{reports/baseline.json} (generated by \path{tools/run_baseline.py})
\end{itemize}

The current Stage 7 check is a simple \cmd{fps >= 30} threshold in the gate output.
Note that real FPS validation is inherently \textbf{hardware-dependent}; CI runs can validate the harness and contracts,
while deployment-relevant benchmarks should be run on reference Linux+NVIDIA hardware.

\section{Real-time constraints and optional features}
\subsection{TTT in real-time}
TTT can improve robustness under domain shift but adds latency.
If you use it in streaming mode, keep it bounded:
\begin{itemize}
  \item prefer \cmd{--ttt-preset safe}
  \item start with \cmd{--ttt-batch-size 1 --ttt-max-batches 1}
  \item prefer \cmd{--ttt-reset stream} when you accept cross-image state
\end{itemize}

For controlled comparisons and ablations, use \cmd{--ttt-reset sample} even though it is slower.

\subsection{Per-detection refinement}
Hessian refinement (depth/rotation/offsets) is usually a \textbf{post-inference research tool}.
It is not typically enabled in production unless the added cost is explicitly budgeted.

\section{Batch throughput tips}
\begin{itemize}
  \item Increase batch size first (until memory limits).
  \item Keep dataset subsets deterministic for comparisons.
  \item Record engine metadata (GPU/driver/TensorRT versions) when benchmarking.
\end{itemize}

\section{Mac development vs GPU execution}
If you develop on macOS, keep GPU/TensorRT runs on a Linux+NVIDIA box.
The repo provides a Runpod-oriented path; see \path{deploy/runpod/README.md}.

\section{What to report in papers/notes}
For real-time or deployment-relevant results, always report:
\begin{itemize}
  \item hardware (GPU, driver, CUDA/TensorRT) and engine build settings
  \item end-to-end latency / FPS and measurement method
  \item whether TTT/refinement/gates were enabled and their bounds
\end{itemize}
