\chapter{Core Concepts and Contracts}

\ChapterMeta{
This chapter explains the core contracts (schemas) and the dataset and predictions migration approach used throughout the repository.
}{
It keeps evaluation stable across backends and over time by making inputs and outputs explicit and versionable.
}{
It assumes you follow the documented schemas and treat generated wrapper artifacts (for example \path{dataset.json}) as part of the workflow.
}

\section{Contract-first evaluation}
The contracts (schemas) are the product. They let you:
\begin{itemize}
  \item compare different inference backends fairly,
  \item reproduce evaluations later,
  \item keep tooling stable even as models evolve.
\end{itemize}

Core contracts:
\begin{itemize}
  \item Predictions schema: \path{docs/predictions_schema.md}
  \item Adapter contract: \path{docs/adapter_contract.md}
  \item Training run contract: \path{docs/run_contract.md}
\end{itemize}

\section{Datasets}
Many tools expect a YOLO-style dataset layout (images/labels/splits) plus optional metadata.

\subsection{Easy migration (use datasets ``as-is'')}
YOLOZU is designed so that common training ecosystems can be evaluated with minimal friction.
The core idea is to avoid rewriting datasets by hand and instead generate small \textbf{descriptor artifacts}.

In practice, migration commands produce:
\begin{itemize}
  \item a small \path{dataset.json} wrapper (points at existing images/labels), and/or
  \item converted label files (e.g., COCO JSON $\rightarrow$ YOLO label txts) plus a wrapper.
\end{itemize}

The goal is to keep source datasets untouched and make evaluation inputs explicit and versionable.

\subsection{Ultralytics datasets (YOLOv8/YOLO11)}
If your dataset is already described by an Ultralytics-style \path{data.yaml} (pointing at \path{images/<split>/} and \path{labels/<split>/}),
YOLOZU can generate a lightweight \path{dataset.json} wrapper while keeping the original dataset untouched.

For the exact command and validation steps, see Chapter~5 (Workflow~D: Migrate datasets and predictions).

\subsection{COCO JSON datasets (migration)}
Many ecosystems use COCO-style \path{instances_*.json} for detection.
YOLOZU consumes YOLO-style label txts, so the supported migration path is:
COCO JSON $\rightarrow$ YOLO labels + \path{dataset.json} wrapper.

For the concrete conversion command, see Chapter~5 (Workflow~D: Migrate datasets and predictions).

\subsection{Importing predictions from other ecosystems}
Detectron2/MMDetection/YOLOX often export COCO detection results.
Typical fields are:
\begin{itemize}
  \item \cmd{image_id}
  \item \cmd{category_id}
  \item \cmd{bbox}
  \item \cmd{score}
\end{itemize}
YOLOZU can convert those into \path{predictions.json}:
\begin{lstlisting}[language=bash]
yolozu migrate predictions \
  --from coco-results \
  --results /path/to/coco_results.json \
  --instances /path/to/instances_val2017.json \
  --output reports/predictions.json \
  --force
\end{lstlisting}

Then validate and evaluate normally.

\section{Predictions JSON}
A predictions artifact is typically a JSON array of per-image objects.
The exact shape depends on task (bbox/seg/pose), but the guiding principles are:
\begin{itemize}
  \item Stable keys and types (schema-validated).
  \item Paths are relative and relocatable when possible.
  \item Metadata is written alongside results (run ID, git SHA, environment).
\end{itemize}

Validate a predictions artifact strictly:
\begin{lstlisting}[language=bash]
python3 tools/validate_predictions.py /path/to/predictions.json --strict
\end{lstlisting}

\section{Adapters (bring-your-own inference)}
An adapter is a thin interface layer between some inference implementation and \yolozu{}'s
contracts.
If you can produce \path{predictions.json}, you can evaluate.
In practice, this means:
\begin{itemize}
  \item input: dataset records provide an \cmd{image} path (and optional label/sidecar metadata),
  \item output: per-image \cmd{detections} with \cmd{class_id}, \cmd{score}, \cmd{bbox} (typically \cmd{cxcywh_norm}), plus task-specific optional fields,
  \item validation: always run \cmd{tools/validate_predictions.py --strict} before evaluation.
\end{itemize}

\section{Reports and reproducibility}
Most workflows are designed to emit:
\begin{itemize}
  \item machine-readable JSON reports,
  \item optional HTML overlays for debugging,
  \item stable artifact naming inside a run directory.
\end{itemize}

This is intentional: it keeps regression testing and comparison easy.
