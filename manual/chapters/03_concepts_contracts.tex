\chapter{Core Concepts and Contracts}

\ChapterMeta{
This chapter elucidates the core contracts (schemas) and the dataset and prediction migration methodologies employed throughout the repository.
}{

It ensures evaluation stability across diverse backends and over time by mandating explicit, versionable inputs and outputs.
}{

This documentation assumes adherence to the specified schemas and treats generated wrapper artifacts (e.g., \path{dataset.json}) as integral components of the workflow.
}

\section{Contract-First Evaluation}
The contracts (schemas) are the foundational product of this repository. They empower you to:
\begin{itemize}
  \item Conduct equitable comparisons across disparate inference backends.
  \item Guarantee the reproducibility of evaluations over time.
  \item Maintain tooling stability, even as underlying models undergo architectural evolution.
\end{itemize}

Core contracts include:
\begin{itemize}
  \item \textbf{Predictions Schema}: \path{docs/predictions_schema.md}
  \item \textbf{Adapter Contract}: \path{docs/adapter_contract.md}
  \item \textbf{Training Run Contract}: \path{docs/run_contract.md}
\end{itemize}

When links and prose disagree, treat these contract documents as canonical and re-verify against \path{tools/manifest.json} and tool \cmd{--help} output.

\section{Datasets}
The majority of tools anticipate a YOLO-style dataset hierarchy (comprising \path{images/}, \path{labels/}, and splits), supplemented by optional metadata.

\subsection{Frictionless Migration (Utilizing Datasets "As-Is")}
\yolozu{} is engineered to evaluate prevalent training ecosystems with minimal friction. The central paradigm is to circumvent manual dataset restructuring by generating lightweight \textbf{descriptor artifacts}.

In practice, migration commands yield:
\begin{itemize}
  \item A concise \path{dataset.json} wrapper (which references existing images and labels), and/or
  \item Converted label files (e.g., translating COCO JSON to YOLO text labels) accompanied by a wrapper.
\end{itemize}

The overarching objective is to preserve the integrity of source datasets while rendering evaluation inputs explicit and versionable.

\subsection{Ultralytics Datasets (YOLOv8/YOLO11)}
If your dataset is already defined by an Ultralytics-style \path{data.yaml} (referencing \path{images/<split>/} and \path{labels/<split>/}), \yolozu{} can generate a lightweight \path{dataset.json} wrapper, leaving the original dataset entirely unmodified.

For precise command syntax and validation procedures, refer to Chapter~5 (Workflow~D: Migrate Datasets and Predictions).

\subsection{COCO JSON Datasets (Migration)}
Numerous ecosystems rely on COCO-style \path{instances_*.json} files for object detection. Because \yolozu{} consumes YOLO-style text labels, the supported migration trajectory is:
COCO JSON \texorpdfstring{$\rightarrow$}{->} YOLO labels + \path{dataset.json} wrapper.

For the specific conversion command, consult Chapter~5 (Workflow~D: Migrate Datasets and Predictions).

\subsection{Importing Predictions from External Ecosystems}
Frameworks such as Detectron2, MMDetection, and YOLOX frequently export detection results in COCO format. Typical fields include:
\begin{itemize}
  \item \cmd{image_id}
  \item \cmd{category_id}
  \item \cmd{box}
  \item \cmd{score}
\end{itemize}

\yolozu{} seamlessly converts these outputs into a compliant \path{predictions.json}:
\begin{lstlisting}[language=bash]
yolozu migrate predictions \
  --from coco-results \
  --results /path/to/coco_results.json \
  --instances /path/to/instances_val2017.json \
  --output reports/predictions.json \
  --force
\end{lstlisting}

Following conversion, proceed with standard validation and evaluation.

\section{Predictions JSON}
A predictions artifact typically manifests as a JSON array containing per-image objects. While the precise structure varies by task (e.g., bounding box, segmentation, pose estimation), the guiding principles remain constant:
\begin{itemize}
  \item \textbf{Stability}: Keys and data types are strictly schema-validated.
  \item \textbf{Portability}: File paths are relative and relocatable wherever feasible.
  \item \textbf{Traceability}: Metadata (such as run ID, Git SHA, and environment details) is embedded alongside the results.
\end{itemize}

To rigorously validate a predictions artifact:
\begin{lstlisting}[language=bash]
python3 tools/validate_predictions.py /path/to/predictions.json --strict
\end{lstlisting}

\section{Adapters (Bring-Your-Own Inference)}
An adapter functions as a thin interface layer bridging a specific inference implementation and \yolozu{}'s standardized contracts. If your system can generate a compliant \path{predictions.json}, it can be evaluated.

In practical terms, this entails:
\begin{itemize}
  \item \textbf{Input}: Dataset records supply an \cmd{image} path (and optional label/sidecar metadata).
  \item \textbf{Output}: Per-image \cmd{detections} must include \cmd{class_id}, \cmd{score}, and \cmd{box} (typically formatted as \cmd{cxcywh_norm}), alongside any task-specific optional fields.
  \item \textbf{Validation}: It is imperative to execute \cmd{tools/validate_predictions.py --strict} prior to evaluation.
\end{itemize}

\section{Reports and Reproducibility}
The majority of workflows are architected to emit:
\begin{itemize}
  \item Machine-readable JSON reports.
  \item Optional HTML overlays for visual debugging.
  \item Stable artifact nomenclature within a designated run directory.
\end{itemize}

This design is highly intentional: it streamlines regression testing and facilitates effortless cross-run comparisons.
