\chapter{Continual Learning (Anti-forgetting)}

\ChapterMeta{
This chapter covers continual fine-tuning across tasks and domains, and the evaluation of forgetting using recorded run artifacts.
}{
It helps mitigate catastrophic forgetting and makes per-task metrics comparable over a training sequence.
}{
It requires continual configs and run artifacts (for example \cmd{continual\_run.json}); optional replay and LoRA settings change compute and forgetting behavior.
}

\section{What YOLOZU means by ``continual''}
Continual learning (CL) here means fine-tuning across a sequence of tasks/domains while mitigating catastrophic forgetting.
In this repository, the reference implementation targets the in-repo \path{rtdetr_pose} scaffold.

\section{Core approach (baseline)}
The default strategy combines:
\begin{itemize}
  \item \textbf{Memoryless self-distillation}: keep the new model close to the previous checkpoint while learning the new task.
  \item \textbf{Optional replay buffer}: add a small buffer of past samples (e.g., reservoir sampling) and train on \emph{(current task + replay)}.
  \item \textbf{Optional parameter-efficiency}: restrict trainable parameters (e.g., LoRA) to reduce forgetting pressure.
\end{itemize}

\section{Main entry points}
\subsection{Train}
Run continual fine-tuning using a config (start from the example):
\begin{lstlisting}[language=bash]
python3 rtdetr_pose/tools/train_continual.py \
  --config configs/continual/rtdetr_pose_domain_inc_example.yaml
\end{lstlisting}

To run \textbf{memoryless} (no replay), set replay size to 0 in the config.

\paragraph{Note on "distillation" terminology}
This chapter's \emph{distillation} refers to \textbf{checkpoint-based self-distillation during training} (e.g., \cmd{--self-distill-from}) as an anti-forgetting mechanism.
The separate tool \path{tools/distill_predictions.py} performs \textbf{offline prediction blending} (rewriting a predictions JSON) and does not by itself mitigate catastrophic forgetting.

\subsection{Evaluate}
Evaluate forgetting and per-task metrics from the recorded run JSON:
\begin{lstlisting}[language=bash]
python3 tools/eval_continual.py \
  --run-json runs/continual/<run>/continual_run.json \
  --device cpu \
  --max-images 50
\end{lstlisting}

If your dataset provides pose sidecar metadata (e.g., under \path{labels/<split>/*.json}), you can request pose metrics:
\begin{lstlisting}[language=bash]
python3 tools/eval_continual.py \
  --run-json runs/continual/<run>/continual_run.json \
  --device cpu \
  --max-images 50 \
  --metric pose \
  --metric-key pose_success
\end{lstlisting}

Available \cmd{--metric-key} values in pose mode:
\begin{itemize}
  \item \cmd{pose\_success}, \cmd{rot\_success}, \cmd{trans\_success}, \cmd{match\_rate}, \cmd{iou\_mean}
  \item \cmd{depth\_abs\_mean}, \cmd{depth\_abs\_median}
  \item \cmd{add\_mean}, \cmd{add\_median}, \cmd{adds\_mean}, \cmd{adds\_median}
\end{itemize}

Notes:
\begin{itemize}
  \item Depth metrics require GT depth/translation and predicted depth/translation fields.
  \item ADD/ADDS metrics require CAD points in sidecar metadata (\cmd{cad\_points} / \cmd{cad\_path} / \cmd{cad\_points\_path}) and valid GT+pred pose.
\end{itemize}

Examples for metric-key switching:
\begin{lstlisting}[language=bash]
# depth error summary
python3 tools/eval_continual.py \
  --run-json runs/continual/<run>/continual_run.json \
  --metric pose \
  --metric-key depth_abs_mean

# ADD / ADDS (requires CAD points)
python3 tools/eval_continual.py \
  --run-json runs/continual/<run>/continual_run.json \
  --metric pose \
  --metric-key add_mean

python3 tools/eval_continual.py \
  --run-json runs/continual/<run>/continual_run.json \
  --metric pose \
  --metric-key adds_mean
\end{lstlisting}

\section{Run artifacts (what to version / what to compare)}
\cmd{train_continual.py} writes a run folder under \path{runs/continual/}.
Key outputs:
\begin{itemize}
  \item \path{continual_run.json}: task list, checkpoints, config hash, and run record.
  \item \path{replay_buffer.json}: buffer summary (when replay is enabled).
  \item Per-task subfolders, typically including:\\
    \path{checkpoint.pt}, \path{metrics.jsonl}, \path{metrics.csv}, \path{run_record.json}.
\end{itemize}

For reporting and plots, treat \path{continual_run.json} as the single source of truth.

\section{Selected config knobs (high leverage)}
Commonly tuned items include:
\begin{itemize}
  \item Replay: \cmd{replay_size}, \cmd{replay_fraction}, \cmd{replay_per_task_cap}.
  \item Distillation: \cmd{distill.enabled}, \cmd{distill.keys}, \cmd{distill.weight}, \cmd{distill.temperature}.
  \item Replay distillation (DER++-style): \cmd{derpp.enabled} and related keys (requires teacher outputs stored in records).
  \item Regularizers across tasks (optional): EWC (\cmd{ewc.*}) and SI (\cmd{si.*}).
\end{itemize}

\section{Research workflow guidance}
\begin{itemize}
  \item \textbf{Pin the evaluation subset}: use a deterministic subset (same images each run) before comparing CL runs.
  \item \textbf{Log runtime cost}: replay and distillation change throughput; report accuracy vs cost.
  \item \textbf{Keep baselines simple}: compare (A) naive fine-tune, (B) memoryless self-distill, (C) replay+distill.
\end{itemize}

\section{Notes}
\begin{itemize}
  \item Some continual comparisons use a CPU-friendly proxy metric (e.g., a lightweight mAP-style summary) on a pinned subset to keep iteration fast.
  \item For ``real'' mAP, switch your evaluation workflow to a full COCO evaluator when appropriate.
\end{itemize}

\section{References (self-distillation background)}
This repository's checkpoint-based self-distillation is a pragmatic anti-forgetting regularizer.
For background reading:
\begin{itemize}
  \item Knowledge distillation (original): Hinton et al., \emph{Distilling the Knowledge in a Neural Network} (arXiv:1503.02531).\;\url{https://arxiv.org/abs/1503.02531}
  \item Self-distillation (classic reference): Furlanello et al., \emph{Born Again Neural Networks} (arXiv:1805.04770).\;\url{https://arxiv.org/abs/1805.04770}
  \item Continual learning via self-distillation: \emph{Self-Distillation Enables Continual Learning} (arXiv:2601.19897).\;\url{https://arxiv.org/abs/2601.19897}
\end{itemize}
