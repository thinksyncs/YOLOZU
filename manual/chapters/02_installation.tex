\chapter{Installation and Setup}

\ChapterMeta{
This chapter walks you through setting up a working \yolozu{} environment and verifying it with quick sanity checks.
}{
It produces a reproducible toolchain and makes backend availability explicit.
}{
It requires Python 3.10 or newer, and optional extras depend on your backend. TensorRT workflows typically require Linux with an NVIDIA GPU.
}

\section{Python requirements}
\yolozu{} targets Python 3.10+.

\section{pip install (user path)}
For most users:
\begin{lstlisting}[language=bash]
python3 -m pip install yolozu
yolozu --help
yolozu doctor --output -
\end{lstlisting}

Optional extras (install only what you need):
\begin{lstlisting}[language=bash]
python3 -m pip install 'yolozu[demo]'    # torch demos
python3 -m pip install 'yolozu[onnxrt]'  # ONNX Runtime tooling
python3 -m pip install 'yolozu[coco]'    # COCOeval support
python3 -m pip install 'yolozu[full]'
\end{lstlisting}

\section{Source checkout (repo path)}
If you are working in the repository (training stack, power-user scripts):
\begin{lstlisting}[language=bash]
python3 -m pip install -r requirements-test.txt
python3 -m pip install -e .
pytest -q
\end{lstlisting}

\section{Recommended deployment flow}
A canonical deployment flow is:
\begin{quote}
PyTorch \(\to\) ONNX \(\to\) TensorRT
\end{quote}

On macOS, you can typically run CPU tooling (validation/eval/export via ONNX Runtime CPU), while
TensorRT paths are usually Linux/NVIDIA-centric.
TensorRT workflows are summarized in the Real-time vs Batch Inference chapter; the canonical pipeline is PyTorch $\rightarrow$ ONNX $\rightarrow$ TensorRT with parity checks.
