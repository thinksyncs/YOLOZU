\chapter{Installation and Setup}

\ChapterMeta{
This chapter guides you through establishing a functional \yolozu{} environment and validating it via preliminary sanity checks.
}{
It ensures the creation of a reproducible toolchain while explicitly detailing backend availability and dependencies.
}{
Python 3.10 or newer is mandatory. Optional extras vary based on the selected backend; notably, TensorRT workflows typically necessitate a Linux environment equipped with an NVIDIA GPU.
}

\section{Python Requirements}
\yolozu{} targets Python 3.10 and newer.

\section{Pip Installation (User Path)}
For standard usage, the recommended installation procedure is:
\begin{lstlisting}[language=bash]
python3 -m pip install yolozu
yolozu --help
yolozu doctor --output -
\end{lstlisting}

\subsection{First successful run (3 commands: install \texorpdfstring{$\rightarrow$}{->} migrate \texorpdfstring{$\rightarrow$}{->} eval)}
For the quickest reproducible success path with evaluation output:
\begin{lstlisting}[language=bash]
python3 -m pip install yolozu
bash scripts/smoke.sh
\end{lstlisting}

The smoke script uses committed assets under \path{data/smoke} and writes
\path{reports/smoke_coco_eval_dry_run.json}.

If your predictions come from another backend, validate first:
\begin{lstlisting}[language=bash]
yolozu validate predictions data/smoke/predictions/predictions_dummy.json --strict
\end{lstlisting}

Optional extras let you install only the dependencies you need for a given workflow:
\begin{lstlisting}[language=bash]
python3 -m pip install 'yolozu[demo]'    # PyTorch demonstration utilities
python3 -m pip install 'yolozu[onnxrt]'  # ONNX Runtime tooling
python3 -m pip install 'yolozu[coco]'    # COCOeval support
python3 -m pip install 'yolozu[full]'    # Comprehensive installation
\end{lstlisting}


\section{Source Checkout (Repository Path)}
If you are operating directly within the repository (e.g., utilizing the training stack or power-user scripts), follow this procedure:
\begin{lstlisting}[language=bash]
python3 -m pip install -r requirements-test.txt
python3 -m pip install -e .
python3 -m pytest -q
\end{lstlisting}


\section{Recommended Deployment Flow}
The canonical deployment pipeline follows this progression:
\begin{quote}
PyTorch \(\to\) ONNX \(\to\) TensorRT
\end{quote}

On macOS, you can typically run CPU-bound tooling (validation, evaluation, and ONNX Runtime CPU export). In contrast, TensorRT execution is generally Linux/NVIDIA-centric.
TensorRT workflows are covered in the Real-Time vs. Batch Inference chapter. The canonical pipeline strictly follows PyTorch $\rightarrow$ ONNX $\rightarrow$ TensorRT and includes parity checks at each stage.
%
