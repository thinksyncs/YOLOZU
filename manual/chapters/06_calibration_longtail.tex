\chapter{Score Calibration and Long-tail Adjustments}

\ChapterMeta{
This chapter describes post-hoc score calibration and long-tail adjustments that transform confidence scores while strictly preserving the underlying geometric predictions.
}{
These methods can improve thresholding behavior and long-tail metrics without retraining, while keeping the transformation reproducible via saved statistics artifacts.
}{
It requires a dataset and a predictions artifact. Method-specific parameters apply, and calibrated outputs should be written as separate artifacts.
}

\section{When calibration is beneficial}
Calibration is useful when your model is accurate but systematically miscalibrated (for example, consistently overconfident or underconfident), or when long-tail classes have suppressed confidence.
In these cases, post-hoc calibration can improve operating points (threshold choice) and class balance \emph{without the cost of retraining}.

\section{The calibrate command}
\yolozu{} provides a unified CLI for several calibration methods.
Conceptually, calibration modifies only \textbf{confidence scores}; the geometric outputs (bounding boxes, masks, keypoints, and pose fields) are left unchanged.
The command writes (1) a calibrated predictions artifact and (2) a small JSON statistics report to make the transformation reproducible.

Example: fit FRACAL statistics for bounding boxes and write calibrated outputs:
\begin{lstlisting}[language=bash]
yolozu calibrate \
  --method fracal \
  --task bbox \
  --dataset /path/to/yolo-dataset \
  --predictions reports/predictions.json \
  --out reports/predictions_calibrated.json \
  --stats-out reports/fracal_stats_bbox.json
\end{lstlisting}

Example: reuse previously computed statistics:
\begin{lstlisting}[language=bash]
yolozu calibrate \
  --method fracal \
  --task bbox \
  --dataset /path/to/yolo-dataset \
  --predictions reports/predictions.json \
  --out reports/predictions_calibrated.json \
  --stats-in reports/fracal_stats_bbox.json
\end{lstlisting}

\section{Supported methods (high-level overview)}
\begin{itemize}
  \item \textbf{FRACAL}: frequency-aware calibration based on class count statistics (the primary in-repo method; recommended for comparisons against standard baselines).
  \item \textbf{LA (Logit Adjustment)}: class-prior logit shift controlled by \cmd{--tau} \cite{menon2021longtail}.
  \item \textbf{NorCal}: frequency-based adjustment controlled by \cmd{--gamma}.
  \item \textbf{Temperature scaling}: global temperature scalar via \cmd{--temperature}, with optional fitting for supported tasks \cite{guo2017calibration}.
\end{itemize}

\section{Task-specific behavior}
Calibration is task-aware:
\begin{itemize}
  \item \textbf{bbox}: adjusts detection confidence scores.
  \item \textbf{seg}: adjusts instance scores while leaving the binary masks unchanged.
  \item \textbf{pose}: adjusts detection scores while preserving keypoint coordinates and pose geometry.
\end{itemize}

\section{Operational best practices}
\begin{itemize}
  \item Keep calibrated outputs as separate artifacts; do not overwrite your baseline predictions.
  \item Record the method name and key parameters (the CLI report is designed to capture this).
  \item Compare methods on the same baseline predictions and the same evaluation split to avoid confounding changes.
\end{itemize}
