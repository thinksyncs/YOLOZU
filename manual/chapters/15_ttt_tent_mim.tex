\chapter{Test-Time Training (TTT): Tent, MIM, CoTTA, EATA, SAR}

\ChapterMeta{
This chapter explains controlled test-time adaptation (Tent/MIM/CoTTA/EATA/SAR), presets, guard rails, and reset policies for fair comparisons.
}{
It can improve robustness under domain shift by adapting weights in memory, while keeping cost bounded and results reproducible.
}{
It is most relevant when there is clear domain shift; comparisons require fixed image sets, bounded \cmd{--ttt-max-batches}, and careful seed and reset policy control.
}

\section{Scope and support}
\yolozu{} supports controlled test-time training (TTT) for the \path{rtdetr_pose} adapter via
\path{tools/export_predictions.py} or the unified wrapper \path{tools/yolozu.py}.

Authoritative references:
\begin{itemize}
  \item \path{docs/ttt_protocol.md}
  \item \path{docs/ttt_integration_plan.md}
  \item \path{docs/mim_inference.md}
  \item \path{docs/eata_design_spec.md}
  \item \path{docs/sar_design_spec.md}
\end{itemize}

TTT updates weights \emph{in memory} using unlabeled test data before (or during) inference \cite{sun2020ttt,wang2021tent}.
This makes comparisons sensitive to domain shift, random seeds, and reset policy.

\section{When TTT is meaningful}
On clean COCO-style validation, TTT can be neutral or harmful.
To demonstrate improvements, you typically need a clear domain shift (e.g., corruptions, style shift, different camera).

Practical baseline:
\begin{itemize}
  \item Baseline domain: clean COCO \cmd{val2017}
  \item Target domain: shifted dataset or corrupted copy of COCO images
\end{itemize}

\section{Presets (start here)}
Both CLIs expose \cmd{--ttt-preset}:
\begin{itemize}
  \item \cmd{safe}: Tent + BN-affine only (\cmd{update_filter=norm_only})
  \item \cmd{adapter_only}: Tent + adapter/head only
  \item \cmd{mim_safe}: MIM + adapter/head only
  \item \cmd{cotta_safe}: CoTTA with conservative update/restore guard rails
  \item \cmd{eata_safe}: EATA selective adaptation with anti-forgetting defaults
  \item \cmd{sar_safe}: SAR LoRA-first sharpness-aware adaptation defaults
\end{itemize}

Presets override core knobs (method, steps, lr, update filter, max batches) and set conservative guard rails unless you override them.

\section{YOLOZU introduction priority (high \texorpdfstring{$\to$}{->} low)}
For \yolozu{} (detection/pose), the recommended operational rollout order is:
\begin{enumerate}
  \item \textbf{CoTTA (highest priority)}: standardize \emph{teacher=EMA(model)}, use \emph{augmentation-averaged predictions} (e.g., flips) as prediction stabilization, and enable \emph{stochastic restoration} to suppress long-run drift.
  Start safely by limiting restoration and updates to \textbf{LoRA/Norm} subsets only.
  \item \textbf{EATA (second)}: add \emph{active sample selection} (only adapt on trusted samples) plus \emph{anti-forgetting regularization} (Fisher/EWC-style protection of important weights).
  In YOLOZU, constrain updates to \textbf{Norm/LoRA/selected head} parameters, and compute selection signals from detection/pose aggregates (e.g., confidence/entropy summaries across queries).
  \item \textbf{SAR (third)}: use \emph{sharpness-aware} entropy minimization for stability in wild mixed-shift / small-batch settings.
  Start with \textbf{LoRA-only} sharpness-aware updates to control cost/side effects, and prefer \textbf{GN/LN} normalization when possible (BN can be unstable under small-batch online adaptation).
\end{enumerate}

\section{CoTTA: operational design for YOLOZU (phase 1)}
This section is the phase-1 CoTTA integration scope for \yolozu{}.
The goal is to suppress long-run drift/forgetting under online test-time adaptation while keeping rollout risk low.

\subsection{Update scope (safe-by-default)}
Phase 1 restricts which parameters can change.

Allowed trainable targets:
\begin{itemize}
  \item LoRA parameters.
  \item normalization affine parameters (LN/GN/BN affine where used).
\end{itemize}

Explicitly disallowed in phase 1:
\begin{itemize}
  \item full backbone weight adaptation,
  \item unrestricted full-model optimizer updates.
\end{itemize}

\subsection{Teacher model: EMA(student)}
CoTTA uses a teacher defined as an exponential moving average (EMA) of the student parameters:
\[
\theta_{teacher} \leftarrow m\,\theta_{teacher} + (1-m)\,\theta_{student}.
\]
Teacher EMA updates run after each adaptation step, with configurable momentum $m$.
Teacher parameters are never directly optimized by gradient descent.

\subsection{Multi-augmentation prediction averaging}
Phase-1 default augmentations:
\begin{itemize}
  \item identity,
  \item horizontal flip.
\end{itemize}

Aggregation behavior:
\begin{itemize}
  \item Run the model on each augmentation branch.
  \item Map predictions back to a common image coordinate system.
  \item Aggregate with a deterministic (seed/config-pinned) confidence-aware averaging rule, then apply the standard postprocess/NMS.
\end{itemize}

\subsection{Stochastic restoration (safe mode)}
Stochastic restoration is applied only to the allowed trainable targets.
For each eligible parameter element, restore from a source snapshot with probability $p_{restore}$.
The source snapshot defaults to the initial pre-adaptation model state for the session.
Restoration executes on a configurable cadence (e.g., every $N$ adaptation steps).

\subsection{Safety boundaries and guardrails}
Phase-1 rollout requires guardrails to prevent runaway adaptation:
\begin{itemize}
  \item gradient norm clipping (\cmd{max_grad_norm}),
  \item per-step update norm cap (\cmd{max_update_norm}),
  \item cumulative update norm cap (\cmd{max_total_update_norm}),
  \item divergence stop condition via loss-ratio threshold (\cmd{max_loss_ratio}).
\end{itemize}

Failure behavior should be explicit and logged:
abort the adaptation step on hard breach, emit a diagnostics event, and restore model state according to the configured fallback policy.

\subsection{Minimum configuration knobs (phase 1)}
To keep comparisons consistent, phase-1 CoTTA should record at least:
\begin{itemize}
  \item \cmd{ttt.method=cotta}
  \item \cmd{ttt.cotta.ema_momentum}
  \item \cmd{ttt.cotta.augmentations} (initial: identity, hflip)
  \item \cmd{ttt.cotta.aggregation} (initial default: confidence-weighted mean)
  \item \cmd{ttt.cotta.restore_prob} and \cmd{ttt.cotta.restore_interval}
  \item \cmd{ttt.update_filter} (must cover norm-only, lora-only, and combined safe subset)
  \item guardrails: \cmd{ttt.max_grad_norm}, \cmd{ttt.max_update_norm}, \cmd{ttt.max_total_update_norm}, \cmd{ttt.max_loss_ratio}
\end{itemize}

\section{Guard rails (safety limits)}
If you do not explicitly set these, defaults are applied by preset:
\begin{itemize}
  \item \cmd{max_grad_norm}
  \item \cmd{max_update_norm}
  \item \cmd{max_total_update_norm}
  \item \cmd{max_loss_ratio}
\end{itemize}

These bounds are there to prevent runaway adaptation and to keep comparisons reproducible.

\section{Reset policy: stream vs sample}
\begin{description}
  \item[\cmd{--ttt-reset stream}] Adapt once (up to \cmd{--ttt-max-batches}) then reuse adapted weights for subsequent images.
  \item[\cmd{--ttt-reset sample}] Restore base state per image, adapt on that image/batch, then predict. Slower but cleaner ablations.
\end{description}

For plots and controlled comparisons, start with \cmd{--ttt-reset sample}.

\section{Bounded-cost knobs}
Two knobs matter most for runtime and fairness:
\begin{itemize}
  \item \cmd{--ttt-batch-size N}: images per adaptation step.
  \item \cmd{--ttt-max-batches K}: hard cap on adaptation batches consumed.
\end{itemize}

Suggested smoke-test settings:
\cmd{--ttt-batch-size 1 --ttt-max-batches 1}.

\section{Method switch (manifest-aligned)}
TTT method can be selected with \cmd{--ttt-method}:
\begin{itemize}
  \item \cmd{tent}
  \item \cmd{mim}
  \item \cmd{cotta}
  \item \cmd{eata}
  \item \cmd{sar}
\end{itemize}

The current canonical CLI surface is reflected in \path{tools/manifest.json}
for \path{tools/export_predictions.py} and \path{tools/yolozu.py}.

\section{Make a fixed evaluation subset}
TTT comparisons must be run on the exact same images.
Use \path{tools/make_subset_dataset.py} to create a deterministic subset dataset root:
\begin{lstlisting}[language=bash]
python3 tools/make_subset_dataset.py \
  --dataset data/coco128 \
  --split train2017 \
  --n 50 \
  --seed 0 \
  --out reports/coco128_50
\end{lstlisting}

This writes \path{subset.json} (including an image hash) and a frozen image list.

\section{Example: baseline vs TTT}
Baseline export:
\begin{lstlisting}[language=bash]
python3 tools/yolozu.py export \
  --backend torch \
  --dataset reports/coco128_50 \
  --split train2017 \
  --checkpoint /path/to.ckpt \
  --device cuda \
  --max-images 50 \
  --output reports/pred_baseline.json
\end{lstlisting}

TTT export (safe preset, per-sample reset, with a log):
\begin{lstlisting}[language=bash]
python3 tools/yolozu.py export \
  --backend torch \
  --dataset reports/coco128_50 \
  --split train2017 \
  --checkpoint /path/to.ckpt \
  --device cuda \
  --max-images 50 \
  --ttt \
  --ttt-preset safe \
  --ttt-reset sample \
  --ttt-log-out reports/ttt_log_safe.json \
  --output reports/pred_ttt_safe.json
\end{lstlisting}

\section{MIM-based adaptation (high level)}
The geometry-aligned MIM branch (when enabled in the model) can be used for test-time adaptation.
Conceptually:
\begin{itemize}
  \item Use geometry-derived features as a teacher (mask + normalized depth).
  \item Reconstruct masked features (MIM/MAE-style) \cite{he2022mae,xie2022simmim} and optionally minimize entropy \cite{grandvalet2005entropy}.
  \item Update a restricted parameter subset (e.g., norm or adapter/head) under guard rails.
\end{itemize}

Conceptually, MIM-style inference adds an auxiliary loss (computed at inference time) to adapt parameters or latent state under distribution shift.
In this repo, the important operational points are:
\begin{itemize}
  \item keep the loss bounded and the update schedule explicit,
  \item record adaptation settings into run artifacts,
  \item compare against a non-adapted baseline under the same pinned evaluation protocol.
\end{itemize}

\section{Operational notes}
\begin{itemize}
  \item TTT adds latency. Always report throughput and the chosen \cmd{batch\_size/max\_batches}.
  \item Keep the evaluation subset fixed and record the exact preset and guards.
  \item For deployment, treat TTT as an optional mode; default inference should not require it.
\end{itemize}

\section{Method-specific evaluation tools}
Manifest-registered benchmark tools for TTT phase validation:
\begin{itemize}
  \item \path{tools/eval_cotta_drift.py} (baseline vs CoTTA drift/stability)
  \item \path{tools/benchmark_eata_stability.py} (baseline vs EATA stability/efficiency)
  \item \path{tools/benchmark_sar_robustness.py} (SAR vs CoTTA/EATA go/no-go impact)
\end{itemize}

These tools emit reproducible JSON/Markdown evidence artifacts and should be preferred over ad-hoc notebook-only comparisons.
