\chapter{Test-Time Training (TTT): Tent, MIM, CoTTA, EATA, SAR}

\ChapterMeta{
This chapter explains controlled test-time adaptation (Tent/MIM/CoTTA/EATA/SAR), presets, guard rails, and reset policies for fair comparisons.
}{
It can improve robustness under domain shift by adapting weights in memory, while keeping cost bounded and results reproducible.
}{
It is most relevant when there is clear domain shift; comparisons require fixed image sets, bounded \cmd{--ttt-max-batches}, and careful seed and reset policy control.
}

\section{Scope and support}
\yolozu{} supports controlled test-time training (TTT) for the \path{rtdetr_pose} adapter via
\path{tools/export_predictions.py} or the unified wrapper \path{tools/yolozu.py}.

Authoritative references:
\begin{itemize}
  \item \path{docs/ttt_protocol.md}
  \item \path{docs/ttt_integration_plan.md}
  \item \path{docs/mim_inference.md}
\end{itemize}

TTT updates weights \emph{in memory} using unlabeled test data before (or during) inference \cite{sun2020ttt,wang2021tent}.
This makes comparisons sensitive to domain shift, random seeds, and reset policy.

\section{When TTT is meaningful}
On clean COCO-style validation, TTT can be neutral or harmful.
To demonstrate improvements, you typically need a clear domain shift (e.g., corruptions, style shift, different camera).

Practical baseline:
\begin{itemize}
  \item Baseline domain: clean COCO \cmd{val2017}
  \item Target domain: shifted dataset or corrupted copy of COCO images
\end{itemize}

\section{Presets (start here)}
Both CLIs expose \cmd{--ttt-preset}:
\begin{itemize}
  \item \cmd{safe}: Tent + BN-affine only (\cmd{update_filter=norm_only})
  \item \cmd{adapter_only}: Tent + adapter/head only
  \item \cmd{mim_safe}: MIM + adapter/head only
  \item \cmd{cotta_safe}: CoTTA with conservative update/restore guard rails
  \item \cmd{eata_safe}: EATA selective adaptation with anti-forgetting defaults
  \item \cmd{sar_safe}: SAR LoRA-first sharpness-aware adaptation defaults
\end{itemize}

Presets override core knobs (method, steps, lr, update filter, max batches) and set conservative guard rails unless you override them.

\section{YOLOZU introduction priority (high \texorpdfstring{$\to$}{->} low)}
For \yolozu{} (detection/pose), the recommended operational rollout order is:
\begin{enumerate}
  \item \textbf{CoTTA (highest priority)}: standardize \emph{teacher=EMA(model)}, use \emph{augmentation-averaged predictions} (e.g., flips) as prediction stabilization, and enable \emph{stochastic restoration} to suppress long-run drift.
  Start safely by limiting restoration and updates to \textbf{LoRA/Norm} subsets only.
  \item \textbf{EATA (second)}: add \emph{active sample selection} (only adapt on trusted samples) plus \emph{anti-forgetting regularization} (Fisher/EWC-style protection of important weights).
  In YOLOZU, constrain updates to \textbf{Norm/LoRA/selected head} parameters, and compute selection signals from detection/pose aggregates (e.g., confidence/entropy summaries across queries).
  \item \textbf{SAR (third)}: use \emph{sharpness-aware} entropy minimization for stability in wild mixed-shift / small-batch settings.
  Start with \textbf{LoRA-only} sharpness-aware updates to control cost/side effects, and prefer \textbf{GN/LN} normalization when possible (BN can be unstable under small-batch online adaptation).
\end{enumerate}

\section{CoTTA: operational design for YOLOZU (phase 1)}
This section is the phase-1 CoTTA integration scope for \yolozu{}.
The goal is to suppress long-run drift/forgetting under online test-time adaptation while keeping rollout risk low.

\subsection{Update scope (safe-by-default)}
Phase 1 restricts which parameters can change.

Allowed trainable targets:
\begin{itemize}
  \item LoRA parameters.
  \item normalization affine parameters (LN/GN/BN affine where used).
\end{itemize}

Explicitly disallowed in phase 1:
\begin{itemize}
  \item full backbone weight adaptation,
  \item unrestricted full-model optimizer updates.
\end{itemize}

\subsection{Teacher model: EMA(student)}
CoTTA uses a teacher defined as an exponential moving average (EMA) of the student parameters:
\[
\theta_{teacher} \leftarrow m\,\theta_{teacher} + (1-m)\,\theta_{student}.
\]
Teacher EMA updates run after each adaptation step, with configurable momentum $m$.
Teacher parameters are never directly optimized by gradient descent.

\subsection{Multi-augmentation prediction averaging}
Phase-1 default augmentations:
\begin{itemize}
  \item identity,
  \item horizontal flip.
\end{itemize}

Aggregation behavior:
\begin{itemize}
  \item Run the model on each augmentation branch.
  \item Map predictions back to a common image coordinate system.
  \item Aggregate with a deterministic (seed/config-pinned) confidence-aware averaging rule, then apply the standard postprocess/NMS.
\end{itemize}

\subsection{Stochastic restoration (safe mode)}
Stochastic restoration is applied only to the allowed trainable targets.
For each eligible parameter element, restore from a source snapshot with probability $p_{restore}$.
The source snapshot defaults to the initial pre-adaptation model state for the session.
Restoration executes on a configurable cadence (e.g., every $N$ adaptation steps).

\subsection{Safety boundaries and guardrails}
Phase-1 rollout requires guardrails to prevent runaway adaptation:
\begin{itemize}
  \item gradient norm clipping (\cmd{max_grad_norm}),
  \item per-step update norm cap (\cmd{max_update_norm}),
  \item cumulative update norm cap (\cmd{max_total_update_norm}),
  \item divergence stop condition via loss-ratio threshold (\cmd{max_loss_ratio}).
\end{itemize}

Failure behavior should be explicit and logged:
abort the adaptation step on hard breach, emit a diagnostics event, and restore model state according to the configured fallback policy.

\subsection{Minimum configuration knobs (phase 1)}
To keep comparisons consistent, phase-1 CoTTA should record at least:
\begin{itemize}
  \item \cmd{ttt.method=cotta}
  \item \cmd{ttt.cotta.ema_momentum}
  \item \cmd{ttt.cotta.augmentations} (initial: identity, hflip)
  \item \cmd{ttt.cotta.aggregation} (initial default: confidence-weighted mean)
  \item \cmd{ttt.cotta.restore_prob} and \cmd{ttt.cotta.restore_interval}
  \item \cmd{ttt.update_filter} (must cover norm-only, lora-only, and combined safe subset)
  \item guardrails: \cmd{ttt.max_grad_norm}, \cmd{ttt.max_update_norm}, \\
    \cmd{ttt.max_total_update_norm}, \cmd{ttt.max_loss_ratio}
\end{itemize}

\section{EATA: operational design for YOLOZU (phase 1)}
This section defines a production-safe phase-1 EATA scope for detection and pose in \yolozu{}.
The goal is to reduce unstable updates by (1) adapting only on informative samples,
(2) limiting update scope to safe parameter subsets, and (3) adding anti-forgetting regularization.

\subsection{Update scope (safe-by-default)}
Allowed trainable targets:
\begin{itemize}
  \item normalization affine parameters (LN/GN/BN affine where used),
  \item LoRA parameters,
  \item optional detection/pose head subset (explicit allowlist only; opt-in).
\end{itemize}

Explicitly disallowed in phase 1:
\begin{itemize}
  \item full backbone updates,
  \item unrestricted full-model adaptation.
\end{itemize}

Phase-1 presets should start with \cmd{norm_only} or \cmd{lora_norm_only} and keep head updates opt-in.

\subsection{Active sample selection (detection/pose aware)}
EATA adaptation steps consume only selected samples from the incoming stream.
For each image, compute selection features from prediction outputs (examples):
\begin{itemize}
  \item detection confidence aggregate (e.g., \cmd{mean_topk_conf}),
  \item detection entropy aggregate (e.g., \cmd{mean_topk_entropy}),
  \item pose confidence aggregate (e.g., \cmd{mean_kpt_conf} when keypoints exist),
  \item optional agreement score across light augmentation branches (phase-1 optional).
\end{itemize}

Phase-1 selection rule (conservative default): select a sample only when all are satisfied:
\begin{itemize}
  \item confidence lower bound: \cmd{conf \ge conf_min},
  \item entropy is within band: \cmd{entropy_min \le entropy \le entropy_max},
  \item valid detection count floor: \cmd{num_valid \ge min_valid_dets}.
\end{itemize}

\subsection{Anti-forgetting regularization (phase 1)}
Phase-1 regularization anchors online updates to the pre-adaptation snapshot.
Use a parameter-anchor penalty on the same trainable subset:
\[
L_{anchor} = \sum_i \lambda_i \left\| \theta_i - \theta_i^{(0)} \right\|_2^2.
\]

Total phase-1 objective is:
\[
L = L_{adapt} + \lambda_{anchor} L_{anchor},
\]
where $L_{adapt}$ is an entropy-based adaptation objective over selected samples.

\subsection{Safety boundaries and guardrails}
Reuse the standard TTT guardrails:
\begin{itemize}
  \item \cmd{max_grad_norm}
  \item \cmd{max_update_norm}
  \item \cmd{max_total_update_norm}
  \item \cmd{max_loss_ratio}
\end{itemize}
and rollback-on-stop behavior.

Additional EATA-specific safety checks:
\begin{itemize}
  \item minimum selected-sample ratio per step (\cmd{selected_ratio_min}),
  \item skip-step when the selected set is empty,
  \item max consecutive skipped steps (\cmd{max_skip_streak}) to trigger early stop with warning.
\end{itemize}

\subsection{Minimum configuration knobs (phase 1)}
Phase-1 EATA should record at least:
\begin{itemize}
  \item \cmd{ttt.method=eata}
  \item \cmd{ttt.update_filter} (\cmd{norm_only}, \cmd{lora_only}, \cmd{lora_norm_only}, optional head allowlist)
  \item thresholds: \\
    \cmd{ttt.eata.conf_min}, \\
    \cmd{ttt.eata.entropy_min}, \\
    \cmd{ttt.eata.entropy_max}, \\
    \cmd{ttt.eata.min_valid_dets}
  \item anchor: \cmd{ttt.eata.anchor_lambda}
  \item selection safety: \cmd{ttt.eata.selected_ratio_min}, \cmd{ttt.eata.max_skip_streak}
  \item guardrails: \cmd{ttt.max_grad_norm}, \cmd{ttt.max_update_norm}, \\
    \cmd{ttt.max_total_update_norm}, \cmd{ttt.max_loss_ratio}
\end{itemize}

\section{SAR: operational design for YOLOZU (phase 1)}
This section defines a production-safe phase-1 SAR rollout scope for \yolozu{}.
The goal is to improve robustness under challenging distribution shifts by introducing sharpness-aware entropy minimization
with controlled update scope.

\subsection{Update scope (LoRA-only by default)}
Allowed update targets:
\begin{itemize}
  \item LoRA parameters only (default).
  \item optional norm affine subset in controlled experiments (not default).
\end{itemize}

Explicitly disallowed in phase 1:
\begin{itemize}
  \item full-backbone sharpness-aware updates,
  \item unrestricted full-model SAM-style optimization.
\end{itemize}

\subsection{Optimization behavior (sharpness-aware entropy)}
Phase-1 SAR uses a two-stage sharpness-aware update over the selected trainable parameters:
\begin{enumerate}
  \item compute gradient on the entropy objective,
  \item perturb weights along a normalized gradient direction,
  \item compute a second loss at the perturbed point,
  \item apply the optimizer update on the original weights.
\end{enumerate}

Conceptually:
\[
\min_{\theta} \max_{\left\| \epsilon \right\| \leq \rho} L_{entropy}(\theta + \epsilon),
\]
with $\rho$ bounded conservatively for phase 1.

\subsection{Normalization policy guidance}
If SAR experiments require normalization updates:
\begin{itemize}
  \item prefer GN/LN-first configurations,
  \item avoid BN-stat-heavy behavior in tiny/unstable batches,
  \item keep BN running-stat side effects monitored and rollback-capable.
\end{itemize}
Default rollout remains LoRA-only.

\subsection{Safety boundaries and expected costs}
Expected cost profile:
approximately $\sim$2x forward/backward cost per adaptation step versus single-step entropy update,
plus extra memory traffic due to perturb-and-restore.

Required safeguards:
strict gradient/update caps, immediate rollback on guardrail breach, explicit stop-reason logging,
and conservative default \cmd{steps=1} and \cmd{max_batches=1}.

\subsection{Minimum configuration knobs (phase 1)}
Phase-1 SAR should record at least:
\begin{itemize}
  \item \cmd{ttt.method=sar}
  \item \cmd{ttt.update_filter} defaulting to \cmd{lora_only}
  \item \cmd{ttt.sar.rho}, \cmd{ttt.sar.adaptive}, \cmd{ttt.sar.first_step_scale}
  \item guardrails: \cmd{ttt.max_grad_norm}, \cmd{ttt.max_update_norm}, \\
    \cmd{ttt.max_total_update_norm}, \cmd{ttt.max_loss_ratio}
\end{itemize}

\section{Guard rails (safety limits)}
If you do not explicitly set these, defaults are applied by preset:
\begin{itemize}
  \item \cmd{max_grad_norm}
  \item \cmd{max_update_norm}
  \item \cmd{max_total_update_norm}
  \item \cmd{max_loss_ratio}
\end{itemize}

These bounds are there to prevent runaway adaptation and to keep comparisons reproducible.

\section{Reset policy: stream vs sample}
\begin{description}
  \item[\cmd{--ttt-reset stream}] Adapt once (up to \cmd{--ttt-max-batches}) then reuse adapted weights for subsequent images.
  \item[\cmd{--ttt-reset sample}] Restore base state per image, adapt on that image/batch, then predict. Slower but cleaner ablations.
\end{description}

For plots and controlled comparisons, start with \cmd{--ttt-reset sample}.

\section{Bounded-cost knobs}
Two knobs matter most for runtime and fairness:
\begin{itemize}
  \item \cmd{--ttt-batch-size N}: images per adaptation step.
  \item \cmd{--ttt-max-batches K}: hard cap on adaptation batches consumed.
\end{itemize}

Suggested smoke-test settings:
\cmd{--ttt-batch-size 1 --ttt-max-batches 1}.

\section{Method switch (manifest-aligned)}
TTT method can be selected with \cmd{--ttt-method}:
\begin{itemize}
  \item \cmd{tent}
  \item \cmd{mim}
  \item \cmd{cotta}
  \item \cmd{eata}
  \item \cmd{sar}
\end{itemize}

The current canonical CLI surface is reflected in \path{tools/manifest.json}
for \path{tools/export_predictions.py} and \path{tools/yolozu.py}.

\section{Make a fixed evaluation subset}
TTT comparisons must be run on the exact same images.
Use \path{tools/make_subset_dataset.py} to create a deterministic subset dataset root:
\begin{lstlisting}[language=bash]
python3 tools/make_subset_dataset.py \
  --dataset data/coco128 \
  --split train2017 \
  --n 50 \
  --seed 0 \
  --out reports/coco128_50
\end{lstlisting}

This writes \path{subset.json} (including an image hash) and a frozen image list.

\section{Example: baseline vs TTT}
Baseline export:
\begin{lstlisting}[language=bash]
python3 tools/yolozu.py export \
  --backend torch \
  --dataset reports/coco128_50 \
  --split train2017 \
  --checkpoint /path/to.ckpt \
  --device cuda \
  --max-images 50 \
  --output reports/pred_baseline.json
\end{lstlisting}

TTT export (safe preset, per-sample reset, with a log):
\begin{lstlisting}[language=bash]
python3 tools/yolozu.py export \
  --backend torch \
  --dataset reports/coco128_50 \
  --split train2017 \
  --checkpoint /path/to.ckpt \
  --device cuda \
  --max-images 50 \
  --ttt \
  --ttt-preset safe \
  --ttt-reset sample \
  --ttt-log-out reports/ttt_log_safe.json \
  --output reports/pred_ttt_safe.json
\end{lstlisting}

\section{MIM-based adaptation (high level)}
The geometry-aligned MIM branch (when enabled in the model) can be used for test-time adaptation.
Conceptually:
\begin{itemize}
  \item Use geometry-derived features as a teacher (mask + normalized depth).
  \item Reconstruct masked features (MIM/MAE-style) \cite{he2022mae,xie2022simmim} and optionally minimize entropy \cite{grandvalet2005entropy}.
  \item Update a restricted parameter subset (e.g., norm or adapter/head) under guard rails.
\end{itemize}

Conceptually, MIM-style inference adds an auxiliary loss (computed at inference time) to adapt parameters or latent state under distribution shift.
In this repo, the important operational points are:
\begin{itemize}
  \item keep the loss bounded and the update schedule explicit,
  \item record adaptation settings into run artifacts,
  \item compare against a non-adapted baseline under the same pinned evaluation protocol.
\end{itemize}

\section{Operational notes}
\begin{itemize}
  \item TTT adds latency. Always report throughput and the chosen \cmd{batch_size/max_batches}.
  \item Keep the evaluation subset fixed and record the exact preset and guards.
  \item For deployment, treat TTT as an optional mode; default inference should not require it.
\end{itemize}

\section{Method-specific evaluation tools}
Manifest-registered benchmark tools for TTT phase validation:
\begin{itemize}
  \item \path{tools/eval_cotta_drift.py} (baseline vs CoTTA drift/stability)
  \item \path{tools/benchmark_eata_stability.py} (baseline vs EATA stability/efficiency)
  \item \path{tools/benchmark_sar_robustness.py} (SAR vs CoTTA/EATA go/no-go impact)
\end{itemize}

These tools emit reproducible JSON/Markdown evidence artifacts and should be preferred over ad-hoc notebook-only comparisons.
