\chapter{Training and the Run Contract}

\ChapterMeta{
This chapter explains the training stack focus and the run contract: stable artifact layout, strict config resolution, and standardized resume.
}{
It enables reproducibility, parity checks, and consistent comparisons across machines and branches by making run outputs predictable.
}{
It assumes you are using run-contract mode (for example \cmd{--run-id} or \cmd{--run-contract}) and respecting strict config keys; training dependencies apply.
}

\section{RT-DETR pose training stack}
The repository includes an RT-DETR-based training pipeline under \path{rtdetr_pose/}.
The important property is not the exact model, but the \textbf{run contract}: stable artifact paths
and resumable runs.

\section{Run contract: why it matters}
A run directory typically contains:
\begin{itemize}
  \item checkpoints (e.g., \path{runs/<run-id>/checkpoints/best.pt}),
  \item exports (e.g., ONNX),
  \item predictions and evaluation reports,
  \item calibration stats artifacts (when enabled).
\end{itemize}

This makes it easy to:
\begin{itemize}
  \item reproduce results,
  \item run parity checks,
  \item compare across branches/machines.
\end{itemize}

Run-contract essentials (codified in the docs and implemented in the trainer):
\begin{itemize}
  \item Enable with \cmd{--run-contract} or \cmd{--run-id <id>}.
  \item Contract mode requires explicit config keys and rejects unknown keys in strict mode.
  \item Fixed artifact layout under \path{runs/<run\_id>/}:
    \path{checkpoints/last.pt}, \path{checkpoints/best.pt},\
    \path{reports/train\_metrics.jsonl}, \path{reports/val\_metrics.jsonl},\
    \path{reports/config\_resolved.yaml}, \path{reports/run\_meta.json}.
  \item Export/parity artifacts are produced in contracted runs by default:\
    \path{exports/model.onnx}, \path{exports/model.onnx.meta.json},\
    \path{reports/onnx\_parity.json}.
  \item Full-state resume is standardized with \cmd{--resume} from
    \path{runs/<run\_id>/checkpoints/last.pt}.
\end{itemize}

\noindent\textbf{Note (low-level trainer):} When invoking \cmd{python3 -m rtdetr_pose.train_minimal} directly,
\cmd{--run-id} implies run-contract mode and requires \cmd{--config} (a YAML/JSON file such as
\path{configs/examples/train_contract.yaml}). For quick smoke runs without the run contract,
use \cmd{--run-dir <path>} and set \cmd{--onnx-out <path>} explicitly when needed.

\section{Config resolution and strictness}
The training entrypoint reads YAML/JSON via \cmd{--config}, then applies explicit CLI flags.
Priority is:
\begin{center}
\textbf{CLI flags \(>\) config file \(>\) built-in defaults}
\end{center}

Config keys must use argparse destination names (example: \cmd{--weight-decay} maps to
\texttt{weight\_decay}). In run-contract mode (\texttt{run\_contract} or \texttt{run\_id} present),
unknown keys are rejected to prevent silent config drift.

\section{YAML workflows: train / resume / test}
The same project can be operated with YAML files for both training and test scenarios.

\begin{longtable}{@{}p{0.18\textwidth}p{0.30\textwidth}p{0.46\textwidth}@{}}
\toprule
\textbf{Workflow} & \textbf{Primary config} & \textbf{Reference command} \\
\midrule
Train (scaffold) & \path{configs/examples/train\_setting.yaml} & \cmd{yolozu train <train\_setting.yaml>} \\
Train (contract) & \path{configs/examples/train\_contract.yaml} & \cmd{yolozu train <train\_contract.yaml> --run-id <id>} \\
Resume (full state) & same as contract & \cmd{yolozu train <train\_contract.yaml> --run-id <id> --resume} \\
Test (scenario runner) & \path{configs/examples/test\_setting.yaml} & \cmd{yolozu test <test\_setting.yaml> [test\_args...]} \\
\bottomrule
\end{longtable}

Example commands:
\begin{lstlisting}[language=bash]
yolozu train configs/examples/train_setting.yaml
yolozu train configs/examples/train_contract.yaml --run-id exp01
yolozu train configs/examples/train_contract.yaml --run-id exp01 --resume
yolozu test configs/examples/test_setting.yaml --adapter precomputed --predictions reports/predictions.json
\end{lstlisting}

\noindent For \cmd{yolozu test}, YAML keys are translated to \cmd{--key value} arguments
(booleans become flags), then extra CLI args override as needed.

\section{Solver (optimizer) parameters and defaults}
Supported optimizers are \texttt{adamw} (default) and \texttt{sgd}.

\begin{longtable}{@{}p{0.30\textwidth}p{0.12\textwidth}p{0.14\textwidth}p{0.38\textwidth}@{}}
\toprule
\textbf{CLI / config key} & \textbf{Type} & \textbf{Default} & \textbf{Notes} \\
\midrule
\cmd{--optimizer} / \texttt{optimizer} & str & \texttt{adamw} & \texttt{adamw}, \texttt{sgd} \\
\cmd{--lr} / \texttt{lr} & float & \texttt{1e-4} & base learning rate \\
\cmd{--weight-decay} / \texttt{weight\_decay} & float & \texttt{0.01} & base weight decay \\
\cmd{--momentum} / \texttt{momentum} & float & \texttt{0.9} & SGD only \\
\cmd{--nesterov} / \texttt{nesterov} & bool & \texttt{false} & SGD only \\
\cmd{--use-param-groups} / \texttt{use\_param\_groups} & bool & \texttt{false} & separate backbone/head groups \\
\cmd{--backbone-lr-mult} / \texttt{backbone\_lr\_mult} & float & \texttt{1.0} & LR multiplier \\
\cmd{--head-lr-mult} / \texttt{head\_lr\_mult} & float & \texttt{1.0} & LR multiplier \\
\cmd{--backbone-wd-mult} / \texttt{backbone\_wd\_mult} & float & \texttt{1.0} & WD multiplier \\
\cmd{--head-wd-mult} / \texttt{head\_wd\_mult} & float & \texttt{1.0} & WD multiplier \\
\cmd{--wd-exclude-bias} / \texttt{wd\_exclude\_bias} & bool & \texttt{true} & bias decay = 0 \\
\cmd{--wd-exclude-norm} / \texttt{wd\_exclude\_norm} & bool & \texttt{true} & norm decay = 0 \\
\bottomrule
\end{longtable}

\section{LR scheduling parameters and defaults}
Supported scheduler values are \texttt{none} (default), \texttt{cosine}, \texttt{onecycle}, and \texttt{multistep}.

\begin{longtable}{@{}p{0.30\textwidth}p{0.12\textwidth}p{0.14\textwidth}p{0.38\textwidth}@{}}
\toprule
\textbf{CLI / config key} & \textbf{Type} & \textbf{Default} & \textbf{Notes} \\
\midrule
\cmd{--scheduler} / \texttt{scheduler} & str & \texttt{none} & \texttt{none}, \texttt{cosine}, \texttt{onecycle}, \texttt{multistep} \\
\cmd{--min-lr} / \texttt{min\_lr} & float & \texttt{0.0} & cosine \texttt{eta\_min} \\
\cmd{--scheduler-milestones} / \texttt{scheduler\_milestones} & str/list & \texttt{""} & comma list for multistep \\
\cmd{--scheduler-gamma} / \texttt{scheduler\_gamma} & float & \texttt{0.1} & multistep decay factor \\
\cmd{--lr-warmup-steps} / \texttt{lr\_warmup\_steps} & int & \texttt{0} & linear warmup steps \\
\cmd{--lr-warmup-init} / \texttt{lr\_warmup\_init} & float & \texttt{0.0} & warmup initial LR \\
\bottomrule
\end{longtable}

\noindent\textbf{Note:} \texttt{linear} is not a supported scheduler value in current code.

\section{LoRA / QLoRA parameters and defaults}
LoRA is enabled when \texttt{lora\_r > 0}.

\begin{longtable}{@{}p{0.30\textwidth}p{0.12\textwidth}p{0.14\textwidth}p{0.38\textwidth}@{}}
\toprule
\textbf{CLI / config key} & \textbf{Type} & \textbf{Default} & \textbf{Notes} \\
\midrule
\cmd{--lora-r} / \texttt{lora\_r} & int & \texttt{0} & \texttt{>0} enables LoRA \\
\cmd{--lora-alpha} / \texttt{lora\_alpha} & float/null & \texttt{null} & null means \(\alpha=r\) \\
\cmd{--lora-dropout} / \texttt{lora\_dropout} & float & \texttt{0.0} & LoRA input dropout \\
\cmd{--lora-target} / \texttt{lora\_target} & str & \texttt{head} & \texttt{head}, \texttt{all\_linear}, \texttt{all\_conv1x1}, \texttt{all\_linear\_conv1x1} \\
\cmd{--lora-freeze-base} / \texttt{lora\_freeze\_base} & bool & \texttt{true} & LoRA-only training \\
\cmd{--lora-train-bias} / \texttt{lora\_train\_bias} & str & \texttt{none} & \texttt{none}, \texttt{all} \\
\cmd{--torchao-quant} / \texttt{torchao\_quant} & str & \texttt{none} & \texttt{none}, \texttt{int8wo}, \texttt{int4wo} \\
\cmd{--torchao-required} / \texttt{torchao\_required} & bool & \texttt{false} & fail if quant backend missing \\
\cmd{--qlora} / \texttt{qlora} & bool & \texttt{false} & requires LoRA, enforces int4wo + freeze \\
\bottomrule
\end{longtable}

\section{Default config list (practical minimum)}
The canonical default list is maintained in:
\begin{itemize}
  \item \path{configs/examples/train_setting.yaml}
  \item \path{configs/examples/train_contract.yaml}
  \item \path{configs/examples/test_setting.yaml}
\end{itemize}

Frequently edited defaults:
\begin{longtable}{@{}p{0.34\textwidth}p{0.20\textwidth}p{0.40\textwidth}@{}}
\toprule
\textbf{Config key} & \textbf{Default} & \textbf{Meaning} \\
\midrule
\texttt{optimizer} & \texttt{adamw} & optimizer family \\
\texttt{lr} & \texttt{1e-4} & base LR \\
\texttt{weight\_decay} & \texttt{0.01} & base WD \\
\texttt{scheduler} & \texttt{none} & LR scheduler mode \\
\texttt{lr\_warmup\_steps} & \texttt{0} & warmup disabled by default \\
\texttt{lora\_r} & \texttt{0} & LoRA disabled by default \\
\texttt{gradient\_accumulation\_steps} & \texttt{1} & no accumulation \\
\texttt{clip\_grad\_norm} & \texttt{0.0} & clipping disabled \\
\texttt{use\_ema} & \texttt{false} & EMA disabled \\
\texttt{amp} & \texttt{none} & AMP disabled \\
\texttt{metrics\_jsonl} & \path{reports/train_metrics.jsonl} & train metric stream \\
\texttt{metrics\_csv} & \path{reports/train_metrics.csv} & tabular metrics \\
\texttt{export\_onnx} & \texttt{true} & ONNX export enabled \\
\texttt{onnx\_out} & \path{reports/rtdetr_pose.onnx} & ONNX output path \\
\bottomrule
\end{longtable}

\section{Smoke training}
A typical smoke entry point is exposed as a script under \path{tools/}.
The exact command may change; search for \path{smoke} scripts in \path{tools/}.

\section{Contracted run example}
\begin{lstlisting}[language=bash]
yolozu train configs/examples/train_contract.yaml --run-id exp01

# Resume from runs/exp01/checkpoints/last.pt
yolozu train configs/examples/train_contract.yaml --run-id exp01 --resume
\end{lstlisting}

\section{Backbone change (model architecture switch)}
Backbone-related settings are not primarily controlled by solver flags; they come from
the model config file referenced by \texttt{model\_config} in the training YAML.

Detailed backbone contract notes and examples are maintained in
\path{docs/backbones.md}.

Backbone output contract is fixed to three feature maps:
\texttt{[P3, P4, P5]} with strides \texttt{[8,16,32]}.
Each level is projected to transformer \texttt{d\_model} by independent 1x1 convs.

Supported backbone names in current code include:
\texttt{cspresnet}, \texttt{tiny\_cnn}, \texttt{cspdarknet\_s},
\texttt{resnet50}, and \texttt{convnext\_tiny}.

Legacy compatibility fields (for example \texttt{backbone\_name},
\texttt{backbone\_channels}) are still accepted, but the preferred form is
\texttt{model.backbone.*} plus \texttt{model.projector.d\_model}.

\begin{longtable}{@{}p{0.28\textwidth}p{0.24\textwidth}p{0.42\textwidth}@{}}
\toprule
\textbf{Where} & \textbf{Key} & \textbf{Role} \\
\midrule
train YAML & \texttt{model\_config} & points to model JSON (example: \path{rtdetr_pose/configs/base.json}) \\
model JSON (\texttt{model.backbone.*}) & \texttt{name} & backbone family (e.g. \texttt{cspresnet}, \texttt{cspdarknet\_s}, \texttt{resnet50}, \texttt{convnext\_tiny}) \\
model JSON (\texttt{model.backbone.*}) & \texttt{norm} & normalization type (\texttt{bn|syncbn|frozenbn|gn}) \\
model JSON (\texttt{model.backbone.*}) & \texttt{args} & optional backbone-specific constructor args \\
model JSON (\texttt{model.projector.*}) & \texttt{d\_model} & projection/output channel size for transformer input \\
solver YAML/CLI & \texttt{backbone\_lr\_mult} & LR multiplier for backbone params \\
solver YAML/CLI & \texttt{backbone\_wd\_mult} & WD multiplier for backbone params \\
\bottomrule
\end{longtable}

Normalization guidance:
\begin{itemize}
  \item \texttt{bn}: default for standard single-node training.
  \item \texttt{syncbn}: useful in multi-GPU runs with small per-rank batch.
  \item \texttt{frozenbn}: stable when batch statistics should not drift.
  \item \texttt{gn}: batch-size-independent fallback.
\end{itemize}

Recommended procedure:
\begin{enumerate}
  \item Copy \path{rtdetr_pose/configs/base.json} to a project-local model JSON.
  \item Edit \texttt{model.backbone.name} and \texttt{model.backbone.args}.
  \item Set \texttt{model.projector.d\_model} to the intended transformer hidden size.
  \item Point training YAML \texttt{model\_config} to the new JSON.
  \item Start with smoke settings (small \texttt{image\_size}/\texttt{max\_steps}), then scale.
\end{enumerate}

Example:
\begin{lstlisting}[language=bash]
cp rtdetr_pose/configs/base.json rtdetr_pose/configs/my_backbone.json
# edit model.backbone.name / model.backbone.args / model.projector.d_model

yolozu train configs/examples/train_setting.yaml \
  --model-config rtdetr_pose/configs/my_backbone.json \
  --use-param-groups \
  --backbone-lr-mult 0.1
\end{lstlisting}
